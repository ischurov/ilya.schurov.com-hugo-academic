{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "unable-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "standing-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sorted-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = Path(\"./index.md\").read_text().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "trained-interview",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---',\n",
       " 'title: Adjoint State Method, Backpropagation and Neural ODEs',\n",
       " '',\n",
       " 'summary: |-',\n",
       " '    You probably heard about Neural ODEs, a neural network architecture based on',\n",
       " '    the ordinary differential equations. To train this kind of models, a mysterious',\n",
       " '    trick called _adjoint state method_ is used. How does it work, why do we',\n",
       " '    need it and how it is related to backpropagation? ',\n",
       " '',\n",
       " 'tags:',\n",
       " '- machine learning',\n",
       " '- differential equations',\n",
       " '- NeuralODE',\n",
       " '',\n",
       " 'date: \"2022-08-05T21:58:00Z\"',\n",
       " '',\n",
       " 'draft: false',\n",
       " '',\n",
       " '# Optional external URL for project (replaces project detail page).',\n",
       " 'external_link: \"\"',\n",
       " '',\n",
       " '# image:',\n",
       " '#   caption: Photo by Michael Daniel ',\n",
       " '#   focal_point: Smart',\n",
       " '# url_code: \"\"',\n",
       " 'url_pdf: \"\"',\n",
       " 'url_slides: \"\"',\n",
       " 'url_video: \"\"',\n",
       " '',\n",
       " '# Slides (optional).',\n",
       " '#   Associate this project with Markdown slides.',\n",
       " \"#   Simply enter your slide deck's filename without extension.\",\n",
       " '#   E.g. `slides = \"example-slides\"` references `content/slides/example-slides.md`.',\n",
       " '#   Otherwise, set `slides = \"\"`.',\n",
       " '---',\n",
       " '',\n",
       " 'You probably heard about Neural ODEs[^1], a neural network architecture based on',\n",
       " 'the ordinary differential equations. When I first read about them, my thoughts were:',\n",
       " 'okay, we have an ODE given by a neural network, now we need to learn weights of',\n",
       " 'that neural network, thus we need gradients, thus we have to find derivates of',\n",
       " 'the solution of an ODE with respect to parameters. And we have a standard tool',\n",
       " \"for that in ODE theory, it's called “variational equations”, so let's just\",\n",
       " \"apply them and we're done. Case closed.\",\n",
       " '',\n",
       " 'However, in the paper, the authors used much more elaborate approach, based',\n",
       " 'on something called _adjoint state method_. It was strange: why we need some cryptic',\n",
       " 'mathematical magic to solve such a standard problem? In various other',\n",
       " 'discussions, I heard from time to time that these mysterious _adjoints_',\n",
       " 'are somehow related to backpropagation. However, all tutorials on adjoint state',\n",
       " 'method I was able to find used a bunch of sophisticated infinite-dimensional',\n",
       " 'optimization theory, and it was not clear for me, how this can be related',\n",
       " 'to such a simple thing as backpropagation? ',\n",
       " '',\n",
       " 'It was my surprise when I understood that adjoint state method in fact is based',\n",
       " 'on a very simple idea. And now I want share it with you.',\n",
       " '',\n",
       " '{{< callout note >}}',\n",
       " '',\n",
       " 'This post turned out to be rather long. I tried to make it as accessible as',\n",
       " 'possible, so included detailed explanations of all the derivations. This led',\n",
       " \"to inclusion of various equations that sometimes can look scary. Don't be afraid:\",\n",
       " 'there are also a lot of illustrations and informal descriptions to guide you',\n",
       " 'through the story.',\n",
       " '',\n",
       " '- In the [first part](#forward-and-backward), I recall briefly some',\n",
       " '    notions from multidimensional calculus and present the main constructions',\n",
       " '    that will be used later. The reader is expected to be familiar with matrix',\n",
       " '    calculations, the notion of a derivative of multidimensional map and the',\n",
       " '    chain rule, whilst the two latter will be recalled.',\n",
       " '',\n",
       " '- The [second part](#backpropagation-in-neural-networks)',\n",
       " '    is an introduction to the backpropagation in the usual dense neural networks. Here I',\n",
       " '    present an exposition that is focused on the effective implementation of the',\n",
       " '    backpropagation using matrix calculations and also has close ties with the',\n",
       " '    adjoint state method in the neural ODEs. Nevertheless, I hope you will find',\n",
       " '    something new and interesting about backpropagation from this part even if you',\n",
       " '    are not interested in the neural ODEs. ',\n",
       " '',\n",
       " '- The [last part](#adjoint-state-method-in-neural-odes) is devoted to the',\n",
       " '    adjoint state method. Here I expect some very basic knowledge of the ordinary',\n",
       " '    differential equations. The main results from the ODE theory will be recalled.',\n",
       " '',\n",
       " 'That is not an easy journey, but I hope you will find it as exciting as I did.',\n",
       " 'If you have any questions regarding this post, do not hesitate to get in touch',\n",
       " 'on',\n",
       " '[Twitter](https://twitter.com/ilya_schurov).',\n",
       " '',\n",
       " \"Now, let's go!\",\n",
       " '',\n",
       " '{{< /callout >}}',\n",
       " '',\n",
       " '## Forward and backward',\n",
       " '',\n",
       " '### How to mupliply matrices',\n",
       " '',\n",
       " \"Before we begin with backpropagation and neural ODEs, let's talk about something\",\n",
       " 'very simple: about matrix multiplication.',\n",
       " '',\n",
       " 'Assume we have two square {{< math >}}$n \\\\times n${{< /math >}} matrices, {{< math >}}$A${{< /math >}} and {{< math >}}$B${{<',\n",
       " '/math >}}, and {{< math >}}$n${{< /math >}}-dimensional vector (vector-column) {{< math >}}$x${{< /math >}}. Consider the',\n",
       " 'following product:',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$ABx$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " \"As matrix multiplication is associatative, we don't need any brackets in this\",\n",
       " \"formula. However, if we try to put them, we'll note that it can be done in two\",\n",
       " 'different ways: we can either write it like this:',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$(AB)x$$',\n",
       " '{{< /math >}} ',\n",
       " '',\n",
       " 'or like this:',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$A(Bx).$$',\n",
       " '{{< /math >}} ',\n",
       " '',\n",
       " \"Of course, we'll get the same results, but computationally these two formulas\",\n",
       " 'are different. In the first case, we find matrix {{< math >}}$AB${{< /math >}},',\n",
       " 'that takes {{< math >}}$O(n^3)${{< /math >}} elementary multiplications, then keep',\n",
       " 'this new matrix in memory, that is {{< math >}}$O(n^2)${{< /math >}}, and then',\n",
       " 'multiply it on {{< math >}}$x${{< /math >}}. The last operation is cheep and',\n",
       " 'only needs {{< math >}}$O(n^2)${{< /math >}} operations.',\n",
       " '',\n",
       " 'In the second approach, we first find {{< math >}}$Bx${{< /math >}}, that is',\n",
       " 'cheap, {{< math >}}$O(n^2)${{< /math >}} operations and {{< math >}}$O(n)${{<',\n",
       " '/math >}} memory. Than we multiply {{< math >}}$A${{< /math >}} by the result of',\n",
       " \"previous computation, that is again cheap. And we're done! So, the difference\",\n",
       " 'between two method is dramatic: {{< math >}}$O(n^3)${{< /math >}} vs. {{< math',\n",
       " '>}}$O(n^2)${{< /math >}} in operations and {{< math >}}$O(n^2)${{< /math >}} vs.',\n",
       " '{{< math >}}$O(n)${{< /math >}} in memory. The second approach is much more',\n",
       " 'efficient!',\n",
       " '',\n",
       " 'Of course, it works only if we have only one vector {{< math >}}$x${{< /math >}} that should be multiplied by {{< math >}}$AB${{< /math >}}; if there are',\n",
       " 'many such vectors, it can be more efficient to find the product {{< math >}}$AB${{< /math >}} once and then reuse it. However, as we will see below, in',\n",
       " 'our problems, including backpropagation, this kind of calculation is effectively',\n",
       " 'one-time. ',\n",
       " '',\n",
       " 'The last thing I want to mention here is that if instead of vector-column $x$ we',\n",
       " 'consider vector-row {{< math >}}$u${{< /math >}} (mathematically speaking, rather ',\n",
       " 'covector than vector, if we represent vectors as vector-columns), and we want to',\n",
       " 'find a product {{< math >}}$uAB${{< /math >}}, this again can be done in two',\n",
       " 'different ways:',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\label{phiAB}',\n",
       " 'uAB=(uA)B=u (AB),',\n",
       " '\\\\end{equation}',\n",
       " '$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " 'and now the first one is much cheaper.',\n",
       " '',\n",
       " 'Does it sounds reasonable? If yes, congratulations: you understood the main idea',\n",
       " 'of the adjoint state method!',\n",
       " '',\n",
       " '### Derivatives and gradients',\n",
       " '',\n",
       " 'In what follows, we will be interested in maps from multidimensional spaces to',\n",
       " 'multiminesional spaces (i.e. from $\\\\mathbb R^n$ to $\\\\mathbb R^m$ for some',\n",
       " 'positive integer $n$ and $m$) and their derivatives. In general, we treat a',\n",
       " 'derivative of a map $f\\\\colon \\\\mathbb R^n \\\\to \\\\mathbb R^m$ as the [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant), i.e.',\n",
       " 'matrix of partial derivatives of components of $f$. We will denote it',\n",
       " 'by $\\\\partial f(x)/\\\\partial x$, sometimes ommitting $(x)$. This matrix has $n$',\n",
       " 'columns and $m$ rows. By definition of a derivative, for any vector $\\\\Delta x \\\\in \\\\mathbb R^n$',\n",
       " 'from some neighborhood of $0$,',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$',\n",
       " 'f(x+\\\\Delta x)-f(x)=\\\\frac{\\\\partial f(x)}{\\\\partial x} \\\\Delta x+o(\\\\|v\\\\|),',\n",
       " '$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " 'or, informally,',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$ f(x+\\\\Delta x)-f(x) \\\\approx \\\\frac{\\\\partial f(x)}{\\\\partial x} \\\\Delta x.',\n",
       " '$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " 'A useful illustration of this approximation is given on [Figure 1](#figure-ill-deriv): for map from',\n",
       " '$\\\\mathbb R$ to $\\\\mathbb R$, the length of an image of the small segment is',\n",
       " 'approximately equal to the derivative multiplied by the length of the segment',\n",
       " 'itself. If we replace segments with vectors based at $x$, the same illustration',\n",
       " 'will work for multidimensional case.',\n",
       " '',\n",
       " '{{< figure src=\"/img/adjoint-state/backprop-14.svg\" width=\"90%\" ',\n",
       " 'title=\"Figure 1. Illustration of a derivative\" id=\"ill-deriv\" >}}',\n",
       " '',\n",
       " 'In a special case when $m=1$, the derivative is a matrix with 1 row, i.e. it is',\n",
       " 'a vector-row. In this case we also call it a *gradient* of $f$ and denote by',\n",
       " '{{< math >}}$\\\\nabla_{\\\\!x} f(x)${{< /math >}}. (Strictly speaking, one should',\n",
       " 'call this vector-row differential, not gradient, because it is a covector, and',\n",
       " \"gradient is a vector, but we'll discuss the\",\n",
       " 'difference between them next time.)',\n",
       " '',\n",
       " 'Let us also recall the well-known ',\n",
       " '[chain rule](https://en.wikipedia.org/wiki/Chain_rule)',\n",
       " 'that simply says that derivative of a composition $g\\\\circ f$ is a product',\n",
       " '(i.e. composition) of the derivatives:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\label{chain-rule}',\n",
       " '\\\\frac{\\\\partial (g\\\\circ f(x))}{\\\\partial x} = \\\\frac{\\\\partial g(h)}{\\\\partial',\n",
       " 'h}\\\\frac{\\\\partial f(x)}{\\\\partial x},',\n",
       " '\\\\end{equation}',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'where the first derivative is taken at point $h=f(x)$.',\n",
       " 'This formula can be easily',\n",
       " 'illustrated with the following picture.',\n",
       " '',\n",
       " '{{< figure src=\"/img/adjoint-state/backprop-15.svg\" width=\"90%\" ',\n",
       " 'title=\"Figure 2. The chain rule\" id=\"chainrule\" >}}',\n",
       " '',\n",
       " 'We will also consider functions that depend on additional multidimensional',\n",
       " 'parameter, usually denoted by $\\\\theta \\\\in \\\\mathbb R^p$. Formally, such a',\n",
       " 'function is just a map',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " 'f\\\\colon \\\\mathbb R^n \\\\times \\\\mathbb R^p \\\\to \\\\mathbb R^m,',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'i.e. it is a function of two vector arguments $f(x, \\\\theta)$, but we usually write it like $f_\\\\theta(x)$ instead. In this case, $\\\\partial f_\\\\theta/\\\\partial x$ is a derivative with',\n",
       " 'respect to argument $x$ (keeping $\\\\theta$ fixed) and $\\\\partial f_\\\\theta / \\\\partial',\n",
       " '\\\\theta$ is a derivative with respect to parameter (keeping $x$ fixed). The',\n",
       " 'following approximations take place:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{gather}',\n",
       " '\\\\label{x-approx}',\n",
       " 'f_\\\\theta(x+\\\\Delta x)-f_\\\\theta(x) \\\\approx \\\\frac{\\\\partial f_\\\\theta(x)}{\\\\partial x} \\\\Delta x,\\\\\\\\',\n",
       " '\\\\label{theta-approx}',\n",
       " 'f_{\\\\theta+\\\\Delta \\\\theta}(x)-f_\\\\theta(x) \\\\approx \\\\frac{\\\\partial f_\\\\theta(x)}{\\\\partial \\\\theta} \\\\Delta \\\\theta.\\\\\\\\',\n",
       " '\\\\end{gather}',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " '### Gradient of composition',\n",
       " '',\n",
       " 'Now let us consider a map $G\\\\colon \\\\mathbb R^n \\\\to \\\\mathbb R$ that can be',\n",
       " 'represented as a composition:',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$G(x)=g^N\\\\circ g^{N-1} \\\\circ\\\\cdots \\\\circ g^1(x),$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " 'where $g^1, \\\\ldots, g^N$ are some differentiable maps from multidimensional spaces',\n",
       " 'to multidimensional spaces. (Superscripts do not denote powers here.) ',\n",
       " '',\n",
       " '{{< callout note >}}',\n",
       " '',\n",
       " 'It is very important for us that codomain of $G$ is one-dimensional. When we',\n",
       " 'will discuss neural networks, $G$ will represent some loss function with values',\n",
       " 'in $\\\\mathbb R$. As $g^N$ is the last applied map, its codomain coincides with',\n",
       " 'the codomain of $G$ and thus it is one-dimensional as well.',\n",
       " '',\n",
       " '{{< /callout >}}',\n",
       " '',\n",
       " '{{< figure src=\"/img/adjoint-state/backprop-5.svg\" width=\"90%\" ',\n",
       " 'title=\"Figure 3. Composition of several functions. All axes except the last one represent multidimensional spaces. The last axis is one-dimensional\" id=\"composition\" >}}',\n",
       " '',\n",
       " 'If we have a value $x$ and want to find $G(x)$, the algorithm is',\n",
       " 'straightforward: we find $h_1:=g^1(x)$, put it into $g^2$, thus finding',\n",
       " '$h_2:=g^2(h_1)$, put it into $g^3$ and so on, the last step is $y=g^N(h_{N-1})$.',\n",
       " 'The flow of calculation is forward, from smaller indexes to larger',\n",
       " '(right-to-left, if we look at the formula, or left-to-right, if we look at the',\n",
       " 'picture).  This is what usually called',\n",
       " '“forward pass” in the neural networks.',\n",
       " '',\n",
       " 'Now what if we want to find the gradient {{< math >}}$\\\\nabla_{\\\\! x}$',\n",
       " '{{< /math >}} (or, in other terms, the derivative $\\\\partial G / \\\\partial x$)?',\n",
       " '',\n",
       " 'The chain rule \\\\eqref{chain-rule} immediately gives us:',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$',\n",
       " '\\\\nabla_{\\\\! x} G=',\n",
       " '\\\\frac{\\\\partial g^N}{\\\\partial h_{N-1}}',\n",
       " '\\\\frac{\\\\partial g^{N-1}}{\\\\partial h_{N-2}}\\\\cdots',\n",
       " '\\\\frac{\\\\partial g^2}{\\\\partial h_1} \\\\frac{\\\\partial g^1}{\\\\partial x}.',\n",
       " '$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " 'As we already said, $g^N$ maps to $\\\\mathbb R$, and therefore one can denote the first multiplier in this product by {{< math >}}$\\\\nabla_{\\\\!h_{N-1}} g^N${{< /math >}}:',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\label{nablaG}',\n",
       " '\\\\nabla_{\\\\! x} G=',\n",
       " '\\\\nabla_{\\\\!h_{N-1}} g^N',\n",
       " '\\\\frac{\\\\partial g^{N-1}}{\\\\partial h_{N-2}}\\\\cdots',\n",
       " '\\\\frac{\\\\partial g^2}{\\\\partial h_1} \\\\frac{\\\\partial g^1}{\\\\partial x}.',\n",
       " '\\\\end{equation}',\n",
       " '$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " 'How to use this equation to find the gradient? First of all, we have to find',\n",
       " '$h_1, \\\\ldots, h_{N-1}$, i.e. perform all the steps of the forward pass (except',\n",
       " 'the last one). Then we have to find all the derivatives of functions',\n",
       " '$g^N$, $g^{N-1}$, …, $g^2$, $g^1$ at the corresponding points $h_{N-1}$, $h_{N-2}$,',\n",
       " '…, $h_1$, $x$.',\n",
       " 'Then we have to multiply everything.',\n",
       " '',\n",
       " 'As the leftmost multiplier is a vector-row, we are in the situation very similar',\n",
       " 'to equation',\n",
       " '$\\\\eqref{phiAB}$: we have a vector-row that is multiplied to a product of',\n",
       " 'matrices. Just like we discussed above, the most natural and efficient way ',\n",
       " 'is to do it left-to-right: we first find a product',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\nabla_{\\\\!h_{N-1}} g^N',\n",
       " '\\\\frac{\\\\partial g^{N-1}}{\\\\partial h_{N-2}},',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'obtain a new vector-row, multiply it by the next matrix, and so on. Now the',\n",
       " 'calculation flow goes backward, from the terms with large indexes to the terms with',\n",
       " 'small indexes (left-to-right if we look at the formula, or right-to-left if we',\n",
       " 'look at the picture). This is what is known as _backward pass_.',\n",
       " '',\n",
       " 'Theoretically, one _can_ find the product in the right-hand side of equation',\n",
       " '$\\\\eqref{nablaG}$ in a different order, e.g. right-to-left, but it would  not be',\n",
       " 'very efficient: one had to find and store some large intermediate matrices',\n",
       " 'during the calculations. In our approach, we store only the initial matrices and',\n",
       " 'intermediate vector-rows.',\n",
       " '',\n",
       " '{{< callout note >}}',\n",
       " '',\n",
       " 'To summarise: consider a function that is given as a',\n",
       " 'composition. There are two natural problems associated with it: to find its',\n",
       " 'value at a particular point and to find its gradient. The flow of calculation',\n",
       " 'of the value is forward, and the flow of calculation of the gradient is',\n",
       " 'backward. To perform backward pass, we need to perform forward pass first to be',\n",
       " 'able to find the derivatives that are needed in the backward pass.',\n",
       " '',\n",
       " '{{< /callout >}}',\n",
       " '',\n",
       " '### Truncated compositions',\n",
       " '',\n",
       " 'It is instructive to study intermediate steps of the forward and the backward passes.',\n",
       " \"Let's begin with the forward pass. \",\n",
       " '',\n",
       " 'For each',\n",
       " 'integer $j$, $0 < j \\\\le N$, ',\n",
       " 'consider the following “truncated” composition:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " 'G^{0:j}(x):=g^j \\\\circ g^{j-1}\\\\circ \\\\cdots \\\\circ g^{1}(x).',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " '',\n",
       " '{{< figure src=\"/img/adjoint-state/backprop-6.svg\" width=\"90%\" ',\n",
       " 'title=\"Figure 4. Forward truncated compositions\" id=\"forward-trunc\"',\n",
       " '>}}',\n",
       " '',\n",
       " 'Each $G^{0:j}$ shows how $h_j$ depends on $x$. In the forward pass, we find',\n",
       " 'consequently $G^{0:1}(x)$, $G^{0:2}(x)$, and so on.  At step $j$ we find',\n",
       " '$G^{0:j}(x)$ applying $g^j$ to the result of the previous step. At the last step',\n",
       " \"$N$ we find $G^{0:N}(x)=G(x)$. That's literally straightforward.\",\n",
       " '',\n",
       " 'To consider backward pass, we need a different “truncation”. For each',\n",
       " 'integer $i$, $0 \\\\le i < N$, let',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\label{truncated}',\n",
       " 'G^{i:N}(h_i):=g^N \\\\circ g^{N-1}\\\\circ \\\\cdots \\\\circ g^{i+1}(h_i).',\n",
       " '\\\\end{equation}',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " '{{< figure src=\"/img/adjoint-state/backprop-7.svg\" width=\"90%\" ',\n",
       " 'title=\"Figure 5. Backward truncated compositions\"',\n",
       " 'id=\"back-trunc\"',\n",
       " '>}}',\n",
       " '',\n",
       " 'It is a map with codomain $\\\\mathbb R$ that shows how $y$ depends on $h_i$. Its gradient can be found using the chain rule:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\nabla_{\\\\! h_{i}} G^{i:N}=',\n",
       " '\\\\nabla_{\\\\!h_{N-1}} g^N',\n",
       " '\\\\frac{\\\\partial g^{N-1}}{\\\\partial h_{N-2}}\\\\cdots',\n",
       " '\\\\frac{\\\\partial g^{i+1}}{\\\\partial h_{i}}.',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'One can see that the right-hand side of this equation is a truncated version of equation $\\\\eqref{nablaG}$: we',\n",
       " 'only keep the first $(N-i)$ multipliers. And this is exactly what backward pass',\n",
       " 'calculates at each step: for each $i$ decreasing from $(N-1)$ to $0$, we find {{< math >}}$ \\\\nabla_{\\\\! h_{i}} G^{i:N}${{< /math >}}',\n",
       " 'multiplying the result of the previous step to $\\\\partial g^{i+1}/\\\\partial h_i$:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\label{nablastep}',\n",
       " '\\\\nabla_{\\\\! h_{i}} G^{i:N}=\\\\nabla_{\\\\! h_{i+1}}G^{i+1:N}\\\\cdot \\\\frac{\\\\partial g^{i+1}}{\\\\partial h_{i}}.',\n",
       " '\\\\end{equation}',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'Clearly, $G^{0:N}=G$ and at the last step we obtain gradient of $G$.',\n",
       " '',\n",
       " 'Here we see that forward and backward passes are very similar in nature, but at',\n",
       " 'the same time has substantial difference. In the forward pass, the domain of',\n",
       " \"each function $G^{0:j}$ we consider is fixed (it's $\\\\mathbb R^n$, the same as the\",\n",
       " 'domain of $G$), but codomain shifts in “forward” direction, see ',\n",
       " '[Figure 4](#figure-forward-trunc). In the backward',\n",
       " \"pass, the codomain of the function $G^{i:N}$ is fixed (it's $\\\\mathbb R$, the\",\n",
       " 'same as the codomain of $G$), but domain shifts in “backward” direction: at step',\n",
       " '$i$, argument of $G^{i:N}$ is $h_i$, and $i$ decreases, see [Figure',\n",
       " '5](#figure-back-trunc).',\n",
       " '',\n",
       " '{{< callout note >}}',\n",
       " '',\n",
       " 'Let us summarise with an informal description. During the calculations, we want',\n",
       " 'to begin with a simple',\n",
       " 'object and transform it to the object we need. In the forward pass, the simple',\n",
       " 'object is just a vector $x$, that “lives” at the “beginning” of the composition.',\n",
       " \"We transform it by application of the corresponding $g^j$'s until we pull it\",\n",
       " 'through the whole composition and get $G(x)$. In the backward pass, the simple',\n",
       " 'object we begin with is the gradient $\\\\nabla_{\\\\\\\\! h_{N-1}} g^{N}$. We can be',\n",
       " \"sure it's “simple” (i.e. a vector-row, not a full matrix) because $g^N$'s\",\n",
       " 'codomain is guaranteed to be one-dimensional. This gradient “lives“ at the “end”',\n",
       " 'of the composition, and it is natural to transform it by extending “backward”.',\n",
       " 'When we pull it through the whole composition, we get the desired gradient',\n",
       " '$\\\\nabla_{\\\\\\\\! x} G$.',\n",
       " '',\n",
       " '{{< /callout >}}',\n",
       " '',\n",
       " '{{< spoiler',\n",
       " 'text=\"Interested in mathematical details? Click here!\" >}}',\n",
       " '',\n",
       " 'I cannot resist the temptation to discuss a bit more mathematical perspective on',\n",
       " 'equation \\\\eqref{nablastep} and add some rigour to the informal description above. ',\n",
       " 'To this end, we have to define formally the spaces',\n",
       " 'where the gradients live.',\n",
       " '',\n",
       " \"Let's say that for each $i=1,\\\\ldots, N$,  $g^i$ is a map from $\\\\mathcal\",\n",
       " 'M_{i-1}=\\\\mathbb R^{n_{i-1}}$ to $\\\\mathcal M_{i}=\\\\mathbb R^{n_i}$, $n_N=1$. As',\n",
       " 'before, $h_i = g^i(h_{i-1})$ and $h_0=x$. The gradient $\\\\nabla_{\\\\\\\\! h_i}',\n",
       " 'G^{i:N}$ is a linear map that acts on vectors $\\\\Delta h_i$. ',\n",
       " 'It is natural to think about this vectors as based at point',\n",
       " '$h_i$. The vector space of all such vectors is called a _tangent space_ of ',\n",
       " '$\\\\mathcal M_{i}$ at point $h_i$; it is denoted by $T_{h_i} \\\\mathcal M_i$. Thus the',\n",
       " 'gradient $\\\\nabla_{\\\\\\\\! h_i} G^{i:N}$ is a linear map from $T_{h_i} \\\\mathcal M_i$',\n",
       " 'to $\\\\mathbb R$, such linear maps (with codomain $\\\\mathbb R$) also known as',\n",
       " '_linear functionals_ or _covectors_. ',\n",
       " '',\n",
       " 'The set of all linear functionals defined on some vector space $V$ is again a',\n",
       " 'vector space: one can add linear functionals to each other and multiply them by',\n",
       " 'real numbers. This space is called _dual space_ to $V$ and denoted by $V^*$. The',\n",
       " \"dual space to the tangent space $T_{h_i} \\\\mathcal M_i$ has a special name: it's\",\n",
       " 'called a _cotangent space_ of $\\\\mathcal M_i$ at point $h_i$ and denoted by',\n",
       " '$T_{h_i}^\\\\* \\\\mathcal M_i$.',\n",
       " '',\n",
       " 'So, the gradient $\\\\nabla_{\\\\\\\\! h_i} G^{i:N}$ belongs to the cotangent space',\n",
       " '$T_{h_i}^\\\\* \\\\mathcal M_i$.',\n",
       " '',\n",
       " \"Now let's consider a derivative of $g^i$ at point $h_{i-1}$.\",\n",
       " 'It is a linear map that transforms vectors based at point $h_{i-1}$ to vectors',\n",
       " \"based at point $h_i$, so it's a map\",\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\frac{\\\\partial g^i(h_{i-1})}{\\\\partial h_{i-1}}\\\\colon T_{h_{i-1}} \\\\mathcal M_{i-1}',\n",
       " '\\\\to T_{h_i} \\\\mathcal M_i.',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'Now I want to consider a very abstract setup that distills the main relations',\n",
       " 'between the objects we introduced so far. We have two vector spaces, denote them',\n",
       " 'by $V$ and $W$, and a linear map ',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\mathcal A\\\\colon V \\\\to W.',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'Consider the dual spaces $V^\\\\*$ and $W^\\\\*$. Then $\\\\mathcal A$ naturally induces',\n",
       " 'a map ',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$\\\\mathcal A^\\\\*\\\\colon W^\\\\* \\\\to V^\\\\*$$ ',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " '(Compare this equation with the equation above. You see:',\n",
       " '$V$ and $W$ are swapped!) For each covector $\\\\psi \\\\in V^*$, we define its image',\n",
       " '$\\\\mathcal A^\\\\* \\\\psi$ with the following formula:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '(\\\\mathcal A^\\\\* \\\\psi)(v)=\\\\psi (\\\\mathcal A v)\\\\quad \\\\text{for each $v\\\\in V$.}',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'What is written here? First, as $\\\\mathcal A^\\\\*$ acts from $W^\\\\*$ to $V^\\\\*$, the',\n",
       " 'image $\\\\mathcal A^\\\\* \\\\psi$ is a covector in $V^\\\\*$, i.e. it is a',\n",
       " 'linear functional defined on $V$. To define this functional, we have to define how',\n",
       " 'it acts on vectors. The value of $\\\\mathcal A^* \\\\psi$ on a vector $v \\\\in V$ is defined',\n",
       " 'in the following way: first, we apply operator $\\\\mathcal A$ to $v$, get a new',\n",
       " 'vector that belongs to $W$, then apply functional $\\\\psi$ (that works on $W$)',\n",
       " 'to this vector. The result is the value of the functional $\\\\mathcal A^\\\\* \\\\psi$',\n",
       " 'on the vector $v$.',\n",
       " '',\n",
       " 'Operator $\\\\mathcal A^\\\\*$ is called an _adjoint_ to $\\\\mathcal A$. If you think about',\n",
       " 'it a little bit, you see that this construction is very-very natural. In fact,',\n",
       " 'it is an example of ',\n",
       " '[contravariant Hom-functor](https://en.wikipedia.org/wiki/Hom_functor) in category theory, but we will not dive into such depths.',\n",
       " '',\n",
       " \"Let's return to our derivatives. Now we can consider an adjoint to the\",\n",
       " 'derivative $\\\\partial g^i / \\\\partial h_{i-1}$:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\left(\\\\frac{\\\\partial g^i(h\\\\_{i-1})}{\\\\partial h\\\\_{i-1}}\\\\right)^\\\\* \\\\colon T^\\\\*\\\\_{h_{i}} \\\\mathcal M\\\\_{i}',\n",
       " '\\\\to T^*\\\\_{h\\\\_{i-1}} \\\\mathcal M\\\\_{i-1}.',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'And equation \\\\eqref{nablastep} takes form:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\nabla_{\\\\\\\\\\\\! h_{i}} G^{i:N}=\\\\left(\\\\frac{\\\\partial g^{i+1}}{\\\\partial h_{i}}\\\\right)^\\\\*',\n",
       " '\\\\nabla_{\\\\\\\\\\\\! h_{i+1}}G^{i+1:N}.',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " '(Just check from the definition of adjoint that this is indeed equivalent to',\n",
       " '\\\\eqref{nablastep}.)',\n",
       " '',\n",
       " 'So, it is the adjoint to the derivative of $g^i$ that acts on the gradients! And as it is',\n",
       " 'an adjoint, it acts “backwards” relative to the action of the derivative itself',\n",
       " '(and thus to the map $g^i$). So it solves the mystery of “backwardness” in',\n",
       " 'backpropagation.',\n",
       " 'Mathematically speaking, we are simply applying contravariant Hom-functor and it',\n",
       " \"reverses all the arrows. That's it!\",\n",
       " '',\n",
       " '{{< /spoiler >}}',\n",
       " '',\n",
       " \"Now let's look how it works in the neural networks.\",\n",
       " '',\n",
       " '## Backpropagation in neural networks',\n",
       " '',\n",
       " 'The backpropagation algorithm is very much well-known, but I',\n",
       " 'present here an exposition that is specifically designed to stress the relation',\n",
       " 'of the backprop and the adjoint state method in the  neural ODEs.',\n",
       " '',\n",
       " '### The usual neural network',\n",
       " '',\n",
       " 'For simplicity, assume we have a neural network that consists only of three',\n",
       " 'layers, two of them are hidden. Layer number {{< math >}}$i${{< /math >}}, ',\n",
       " '{{< math >}}$i=1,2,3${{< /math >}}, transforms its input to output using a',\n",
       " 'function',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '    f^{i}_\\\\theta\\\\colon \\\\mathbb R^{n_{i-1}} \\\\to \\\\mathbb R^{n_i},',\n",
       " '$${{< /math >}} ',\n",
       " '',\n",
       " 'where {{< math >}}$\\\\theta\\\\in \\\\mathbb R^p${{< /math >}}',\n",
       " 'is a vector of all parameters of the neural network (i.e. all weights and',\n",
       " 'biases), {{< math >}}$n_i${{< /math >}} is the dimensionality of the output of',\n",
       " \"{{< math >}}$i${{< /math >}}'th layer, {{< math >}}$n_0${{< /math >}} is the input\",\n",
       " 'dimensionality of the network. Usually each layer depends only',\n",
       " 'on a subset of parameters in {{< math >}}$\\\\theta${{< /math >}} and implements an',\n",
       " 'affine function in elementwise composition with nonlinear activation function,',\n",
       " 'but we are not interested in such architecture details now and consider rather',\n",
       " 'general case. The full network',\n",
       " 'defines a function',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '    f_{\\\\theta}(x) := f^{3}_\\\\theta\\\\circ f^{2}_\\\\theta \\\\circ f^{1}_\\\\theta(x)',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'This is a very similar to that discussed in the [previous',\n",
       " 'section](#gradient-of-composition). The main difference is that now all the',\n",
       " 'functions in this composition depend also on the parameter $\\\\theta$.',\n",
       " '',\n",
       " 'Our composition can be visualized in the following way:',\n",
       " '',\n",
       " '{{< figure src=\"/img/adjoint-state/backprop-1.svg\" width=\"90%\" ',\n",
       " 'title=\"Figure 6. Three-layer neural network\"',\n",
       " 'id=\"three-layer\"',\n",
       " '>}}',\n",
       " '',\n",
       " 'We also have some loss function $L(y, y_{true})$ (e.g. in case of quadratic',\n",
       " 'loss, $L(y, y_{true})=(y-y_{true})^2$). If we put the output of the network into',\n",
       " 'the loss, we obtain an optimization objective',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\mathcal L(\\\\theta) := L(f_\\\\theta(x_{input}), y_{true})',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'that should be minimized during the training.  For simplicity, we are discussing',\n",
       " 'the loss at one datapoint; in the real settings, we would average this over the',\n",
       " 'batch.',\n",
       " '',\n",
       " '### Gradient of the loss',\n",
       " '',\n",
       " 'To perform the optimization of $\\\\mathcal L(\\\\theta)$ with gradient descent, one need to find its gradient. Chain rule immediately gives:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\label{nablamathcalL}',\n",
       " '\\\\nabla_{\\\\!\\\\theta} \\\\mathcal L(\\\\theta) = \\\\nabla_{\\\\!y} L \\\\cdot \\\\frac{\\\\partial',\n",
       " 'f_\\\\theta(x_{input})}{\\\\partial \\\\theta},',\n",
       " '\\\\end{equation}',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'where the first multiplier is a gradient of {{< math >}}$L${{< /math >}}, i.e.',\n",
       " 'vector-row of dimensionality {{< math >}}$n_3${{< /math >}} (dimensionality of',\n",
       " 'the output layer), and the second multiplier is a ',\n",
       " '{{< math >}}$(n_3 \\\\times p)${{< /math >}}-matrix.',\n",
       " '',\n",
       " 'It is easy to find {{< math >}}$\\\\nabla_{\\\\!y} L ${{< /math >}} provided that {{< math',\n",
       " '>}}$y${{< /math >}} is already calculated (i.e. in the case of quadratic loss,',\n",
       " \"it's just {{< math >}}$(2y-2y_{true})${{< /math >}}). To find the second\",\n",
       " 'multiplier, one have to decompose {{< math >}}$f_\\\\theta${{< /math >}} into a',\n",
       " 'composition of subsequent layer maps and again apply the chain rule. In contrast',\n",
       " 'with the [previous part](#gradient-of-composition), each function now depends not only on its argument, but',\n",
       " \"also on the parameter $\\\\theta$. This leads to new phenomena and I'd like to\",\n",
       " 'study it with some not-so-rigorous visualization.',\n",
       " '',\n",
       " \"Let's fix some small vector {{< math >}}$\\\\Delta \\\\theta \\\\in \\\\mathbb R^p${{< /math >}} and consider a “trajectory” of $x_{input}$ under the action of the “perturbed” maps\",\n",
       " '{{< math >}}$f^{i}_{\\\\theta+\\\\Delta \\\\theta}${{< /math >}}, {{< math >}}$i=1,2,3${{<',\n",
       " '/math >}}:',\n",
       " '',\n",
       " '{{< figure src=\"/img/adjoint-state/backprop-2.svg\" width=\"90%\" ',\n",
       " 'title=\"Figure 7. What happens with the output of neural network if we slighly change parameters.\"',\n",
       " 'id=\"nn-change\"',\n",
       " '>}}',\n",
       " '',\n",
       " 'The difference between outputs {{< math >}}$f_{\\\\theta+\\\\Delta',\n",
       " '\\\\theta}(x_{input})-f_\\\\theta(x_{input})${{< /math >}} is approximately equal to',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\frac{\\\\partial f_\\\\theta}{\\\\partial \\\\theta} \\\\Delta \\\\theta',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'provided that {{< math >}}$\\\\Delta \\\\theta${{< /math >}} is small by the definition of the derivative, see equation \\\\eqref{theta-approx}. (Note that on',\n",
       " 'the picture this difference is represented by a segment on a line, but in',\n",
       " \"reality it's a {{< math >}}$n_3${{< /math >}}-dimensional vector.) \",\n",
       " '',\n",
       " '### Derivative of the network',\n",
       " '',\n",
       " \"Now let's decompose this difference into a sum of three parts in the following\",\n",
       " 'way (see [Figure 8](#figure-decomp-net) below). For each of the intermediate points of the unperturbed',\n",
       " 'trajectory (i.e. $f^1_\\\\theta(x_{input})$ and $f^2_\\\\theta\\\\circ f^1_\\\\theta(x_{input})$), we',\n",
       " 'consider a trajectory of the perturbed network that starts from this point.',\n",
       " 'These trajectories split the segment $[f_{\\\\theta}(x_{input}),',\n",
       " 'f_{\\\\theta+\\\\Delta \\\\theta}(x_{input})]$ into three smaller segments denoted (from top to bottom)',\n",
       " 'by {{< math >}}$\\\\Delta^3_1${{< /math >}}, {{< math >}}$\\\\Delta^3_2${{< /math >}}',\n",
       " 'and {{< math >}}$\\\\Delta^3_3${{< /math >}}.',\n",
       " '',\n",
       " '{{< figure src=\"/img/adjoint-state/backprop-4.svg\" width=\"90%\" ',\n",
       " 'title=\"Figure 8. Decomposition of the network\\'s derivative\"',\n",
       " 'id=\"decomp-net\"',\n",
       " '>}}',\n",
       " '',\n",
       " 'Here all the red arrows represent the action of the corresponding {{< math',\n",
       " '>}}$f^i_{\\\\theta+\\\\Delta \\\\theta}${{< /math >}}. ',\n",
       " '',\n",
       " '{{< callout note >}}',\n",
       " '',\n",
       " 'Of course, this is not an exact figure: in reality, the output space is',\n",
       " 'multidimensional, and we do not split a segment into smaller segments.',\n",
       " 'Nevertheless, the argument is correct: we can represent a vector from',\n",
       " '$f_\\\\theta(x_{input})$ to $f_{\\\\theta+\\\\Delta \\\\theta}(x_{input})$ as a sum of three',\n",
       " 'vectors given as a difference between the values of the corresponding',\n",
       " 'compositions. So, no cheating here!',\n",
       " '',\n",
       " '{{< /callout >}}',\n",
       " '',\n",
       " 'We will approximate each of the smaller parts using the appropriate derivatives.',\n",
       " \"Let's begin with {{< math >}}$\\\\Delta^3_3${{< /math >}}. It measures the\",\n",
       " 'difference between the images of some point under action of {{< math',\n",
       " '>}}$f^3_{\\\\theta+\\\\Delta \\\\theta}${{< /math >}} and {{< math >}}$f^3_{\\\\theta}${{<',\n",
       " '/math >}}. Again, we use the definition of a derivate (particularly, equation',\n",
       " '\\\\eqref{theta-approx}) and get the following',\n",
       " 'approximation:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\Delta^3_3 \\\\approx \\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} \\\\Delta \\\\theta.',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'That was easy. Now consider {{< math >}}$\\\\Delta^3_2${{< /math >}}. Here we have',\n",
       " 'two steps. At the first step, we have two functions, {{< math >}}$f^2_\\\\theta${{<',\n",
       " '/math >}} and {{< math >}}$f^2_{\\\\theta+\\\\Delta \\\\theta}${{< /math >}} that are applied to the same point. The difference between the images is denoted by {{< math >}}$\\\\Delta^2_2${{< /math >}} and like in the previous case is approximately equal to ',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\Delta^2_2 \\\\approx \\\\frac{\\\\partial f^2_{\\\\theta}}{\\\\partial \\\\theta} \\\\Delta \\\\theta.',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'At the second step, we have ',\n",
       " 'one function, {{< math >}}$f^2_{\\\\theta+\\\\Delta \\\\theta}${{< /math >}}, that is',\n",
       " 'applied to two different points. To find the difference between the images now,',\n",
       " 'we have to use the derivative of {{< math >}}$f^3_{\\\\theta+\\\\Delta \\\\theta}(h_2)${{< /math >}} with',\n",
       " 'respect to its argument {{< math >}}$h_2${{< /math >}}, see equation',\n",
       " '\\\\eqref{x-approx}. Namely:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\Delta^3_2 \\\\approx ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta + \\\\Delta \\\\theta}}{\\\\partial h_2}\\\\Delta^2_2 \\\\approx ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta}}{\\\\partial \\\\theta} \\\\Delta \\\\theta.',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'And finally for {{< math >}}$\\\\Delta^3_1${{< /math >}} we have three steps:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\Delta^3_1 \\\\approx ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta + \\\\Delta \\\\theta}}{\\\\partial h_2}\\\\Delta^2_1 \\\\approx ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_1} \\\\Delta_1^1 \\\\approx',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial',\n",
       " '\\\\theta} \\\\Delta \\\\theta.',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " \"Now let's sum up everything:\",\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{align*}',\n",
       " '\\\\frac{\\\\partial f_{\\\\theta}}{\\\\partial \\\\theta}\\\\Delta \\\\theta \\\\approx {} & \\\\Delta^3_3 +',\n",
       " '\\\\Delta^3_2 + \\\\Delta^3_1 \\\\approx  \\\\\\\\',\n",
       " '& \\\\left(',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} + ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta}}{\\\\partial \\\\theta} +',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial',\n",
       " '\\\\theta}\\\\right) \\\\Delta \\\\theta.',\n",
       " '\\\\end{align*}',\n",
       " '$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " 'As $\\\\Delta \\\\theta$ tends to zero, the approximations become more and more precise,',\n",
       " 'and now one can easily believe that',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\label{partialftheta}',\n",
       " '\\\\frac{\\\\partial f_{\\\\theta}}{\\\\partial \\\\theta} = ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} + ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta}}{\\\\partial \\\\theta} +',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial',\n",
       " '\\\\theta}.',\n",
       " '\\\\end{equation}',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'We used a lot of informal derivations with “approximate equal” signs that does',\n",
       " 'not count as a rigorous proof. (Do not try to sell it to your Calculus professor,',\n",
       " \"unless it's me!) They can be easily replaced with several\",\n",
       " 'applications of the chain rule, but I want to make clear where each term in this',\n",
       " 'formula came from, and it was easier to do that with the informal picture. ',\n",
       " '',\n",
       " '{{< callout note >}}',\n",
       " '',\n",
       " \"Let's look at the last formula again. We see that to find a derivative of the network with respect to the parameter $\\\\theta$, we have to account for two effects: \",\n",
       " '',\n",
       " '1. Change of the parameter $\\\\theta$ affects output of a particular layer. ',\n",
       " '',\n",
       " '2. Change of the output of a layer affects outputs of the subsequent layers, even if we ignore',\n",
       " 'change of the parameter for them. ',\n",
       " '',\n",
       " 'The first effect is addressed by $\\\\partial f_\\\\theta^i / \\\\partial \\\\theta$',\n",
       " 'multipliers. The second effect is addressed by $\\\\partial f_\\\\theta^i / \\\\partial',\n",
       " 'h^{i-1}$ multipliers. The derivative is a sum of the corresponding effects for',\n",
       " 'each layer.',\n",
       " '',\n",
       " '{{< /callout >}}',\n",
       " '',\n",
       " '### Back to the gradient',\n",
       " '',\n",
       " \"Now let's use the equation for the derivative of $f$ to find a gradient of $\\\\mathcal\",\n",
       " 'L$. We put $\\\\eqref{partialftheta}$ to $\\\\eqref{nablamathcalL}$ and obtain:',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$',\n",
       " '\\\\begin{align*}',\n",
       " '\\\\nabla_{\\\\!\\\\theta} \\\\mathcal L= {} &\\\\nabla_{\\\\!y} L \\\\cdot \\\\frac{\\\\partial',\n",
       " 'f_\\\\theta}{\\\\partial \\\\theta}=\\\\\\\\',\n",
       " '& \\\\nabla_{\\\\!y} L \\\\cdot \\\\left(',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} + ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta}}{\\\\partial \\\\theta} +',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial',\n",
       " '\\\\theta}\\\\right)=\\\\\\\\',\n",
       " '& ',\n",
       " '\\\\nabla_{\\\\!y} L \\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} + ',\n",
       " '{\\\\nabla_{\\\\!y} L ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2}}',\n",
       " '\\\\frac{\\\\partial f^2_{\\\\theta}}{\\\\partial \\\\theta} +',\n",
       " '{\\\\nabla_{\\\\!y} L ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2}} ',\n",
       " '\\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial',\n",
       " '\\\\theta}.',\n",
       " '\\\\end{align*}',\n",
       " '$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " 'Note the familiar pattern? In each term, we have vector-row {{< math',\n",
       " '>}}$\\\\nabla_{\\\\!y}L${{< /math >}} that is multiplied by a sequence of matrices.',\n",
       " 'That means we need to multiply it left-to-right. Moreover, if we look closer, we',\n",
       " 'see there are common parts in the second and the third summands:',\n",
       " '',\n",
       " '{{< math >}}',\n",
       " '$$',\n",
       " '\\\\begin{align}',\n",
       " '\\\\nonumber',\n",
       " '\\\\nabla_{\\\\!\\\\theta} \\\\mathcal L(\\\\theta)= ',\n",
       " '\\\\nabla_{\\\\!y} L \\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} & + ',\n",
       " '{\\\\color{teal}\\\\left(\\\\nabla_{\\\\!y} L ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2}\\\\right)}',\n",
       " '\\\\frac{\\\\partial f^2_{\\\\theta}}{\\\\partial \\\\theta} \\\\\\\\',\n",
       " '\\\\label{nablamcL}',\n",
       " '&+ {\\\\color{teal} \\\\left(\\\\nabla_{\\\\!y} L ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2}\\\\right)} ',\n",
       " '\\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial',\n",
       " '\\\\theta}.',\n",
       " '\\\\end{align}',\n",
       " '$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " 'It means that we can find this common part {{< math >}}$\\\\nabla_{\\\\!y} L ',\n",
       " '\\\\cdot \\\\partial f^3_{\\\\theta}/\\\\partial h_2${{< /math >}} when calculate the second',\n",
       " \"summand, and then reuse it when calculating the third summand. That's allows us\",\n",
       " 'to do the calculations even more efficiently. And this is not a coincidence: the',\n",
       " 'same trick works in deeper networks as well!',\n",
       " '',\n",
       " '### General algorithm for backpropagation',\n",
       " '',\n",
       " 'Previously we considered a network with three layers. Now I want to',\n",
       " 'generalize the formula for loss gradient to the general case of the network with $N$ layers.',\n",
       " '',\n",
       " 'Note that in each summand on the right-hand side of equation $\\\\eqref{nablamcL}$ only',\n",
       " 'the last multiplier is a derivative with respect to the parameters $\\\\theta$. The',\n",
       " 'beginning part of each product is a gradient of “truncated composition” like in',\n",
       " '$\\\\eqref{truncated}$ with respect to the output of some of the hidden layer.',\n",
       " 'Indeed, the chain rule implies:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\nabla_{\\\\!y} L ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} = ',\n",
       " '\\\\nabla_{\\\\!h_2}(L\\\\circ f^3_\\\\theta)',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'and',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\nabla_{\\\\!y} L ',\n",
       " '\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial f^2_\\\\theta}{\\\\partial',\n",
       " 'h_1} = ',\n",
       " '\\\\nabla_{\\\\!h_1}(L\\\\circ f^3_\\\\theta \\\\circ f^2_\\\\theta).',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'In other words, these multipliers show how the loss function depends on the',\n",
       " 'output value of the second and the first hidden layers correspondingly. To',\n",
       " 'simplify the notation, we will write {{< math >}}$\\\\nabla_{\\\\!h_2} L${{< /math >}} and {{< math >}}$\\\\nabla_{\\\\!h_1} L${{< /math >}}',\n",
       " 'and omit the subsequent compositions with the layer maps. ',\n",
       " '',\n",
       " 'Let us denote $h_3 \\\\equiv y$. Then the following nice relations take place:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{align}',\n",
       " '\\\\label{nablaLstep1}',\n",
       " '\\\\nabla_{\\\\!h_2} L & = \\\\nabla_{\\\\!h_3} L \\\\cdot  ',\n",
       " '    \\\\frac{\\\\partial f^3_\\\\theta}{\\\\partial h_2}, \\\\\\\\',\n",
       " '\\\\label{nablaLstep2}',\n",
       " '\\\\nabla_{\\\\!h_1} L & = \\\\nabla_{\\\\!h_2} L \\\\cdot ',\n",
       " '    \\\\frac{\\\\partial f^2_\\\\theta}{\\\\partial h_1}.',\n",
       " '\\\\end{align}',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'This is actually just a restatement of the general equation',\n",
       " '$\\\\eqref{nablastep}$ for truncated compositions.',\n",
       " '',\n",
       " 'With this new notation, we can rewrite the formula for the gradient',\n",
       " '$\\\\eqref{nablamcL}$ in the following compact way:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\nabla_{\\\\!\\\\theta} \\\\mathcal L(\\\\theta)=',\n",
       " '\\\\nabla_{\\\\!h_3} L \\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} + ',\n",
       " '\\\\nabla_{\\\\!h_2} L ',\n",
       " '\\\\frac{\\\\partial',\n",
       " 'f^2_{\\\\theta}}{\\\\partial \\\\theta} +',\n",
       " '\\\\nabla_{\\\\!h_1} L ',\n",
       " '\\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial',\n",
       " '\\\\theta}.',\n",
       " '$$',\n",
       " '{{< /math >}}',\n",
       " '',\n",
       " 'And this can be easily generalized to the case of $N$ layers:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\label{nabla-L-sum}',\n",
       " '\\\\nabla_{\\\\!\\\\theta} \\\\mathcal L(\\\\theta)=\\\\sum_{i=N}^1 \\\\nabla_{\\\\!h_i}L \\\\frac{\\\\partial',\n",
       " 'f_\\\\theta^i}{\\\\partial \\\\theta},',\n",
       " '\\\\end{equation}',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'where $h_N\\\\equiv y$. (I am intentionally start the summation from $i=N$ and then',\n",
       " 'decrease $i$ until it equals $1$ for consistency with the previous equation and',\n",
       " 'the algorithm below.) This equation looks simple, and, moreover, there exists',\n",
       " 'efficient algorithm to calculate its right-hand side. First, note that equations',\n",
       " '$\\\\eqref{nablaLstep1}$-$\\\\eqref{nablaLstep2}$ are immediately generalized as',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{equation}',\n",
       " '\\\\label{nablaLstep}',\n",
       " '\\\\nabla_{\\\\!h_i} L  = \\\\nabla_{\\\\!h_{i+1}} L \\\\cdot  ',\n",
       " '    \\\\frac{\\\\partial f^{i+1}_\\\\theta}{\\\\partial h_i}, \\\\quad i = N, \\\\ldots, 1.',\n",
       " '\\\\end{equation}',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " '(Again, this is just equation $\\\\eqref{nablastep}$ with different notation.)',\n",
       " '',\n",
       " 'Then we have the following algorithm:',\n",
       " '',\n",
       " '{{< callout note >}}',\n",
       " '',\n",
       " '1. Do the forward pass to find values $h_1$, $h_2$, …, $h_N\\\\equiv y$.',\n",
       " '',\n",
       " '2. Initialize the accumulator to store the gradient with zero $p$-dimensional',\n",
       " '   vector-row. (Recall that $p$ is the number of parameters.)',\n",
       " '',\n",
       " '3. Find {{< math >}}$\\\\nabla_{\\\\\\\\!y} L(y)${{< /math >}}.',\n",
       " '',\n",
       " '4. Find {{< math >}}$(\\\\nabla_{\\\\\\\\!y} L) (\\\\partial f^N_\\\\theta(h_{N-1}) / \\\\partial \\\\theta)${{< /math >}} and add it to the accumulator.',\n",
       " '',\n",
       " '5. For each $i$ from $(N - 1)$ to $1$:',\n",
       " '',\n",
       " '    - Find {{< math >}}$\\\\nabla_{\\\\\\\\!h_i} L${{< /math >}} by multiplication of',\n",
       " '        the previously found {{< math >}}$\\\\nabla_{\\\\\\\\!h_{i+1}} L${{< /math >}} to the',\n",
       " '        derivative {{< math >}}$\\\\partial f^{i+1}_\\\\theta(h\\\\_{i}) / \\\\partial h\\\\_{i}${{< /math >}}',\n",
       " '',\n",
       " '    - Find {{< math >}}$(\\\\nabla_{\\\\\\\\! h_i} L) (\\\\partial f^{i}\\\\_\\\\theta(h_{i-1}) / \\\\partial \\\\theta)${{< /math >}} and add it to the accumulator.',\n",
       " '',\n",
       " '6. Return the value of the accumulator.',\n",
       " '',\n",
       " '{{< /callout >}}',\n",
       " '',\n",
       " \"That's it. That's how backpropagation allows to efficiently calculate gradients\",\n",
       " \"in the usual neural networks. Now let's pass to neural ODEs.\",\n",
       " '',\n",
       " '## Adjoint State Method in Neural ODEs',\n",
       " '',\n",
       " '### Neural ODEs: quick recap',\n",
       " '',\n",
       " 'As we discussed previously, during the forward pass, the usual neural network',\n",
       " 'transforms its inputs to outputs in a sequence of discrete steps: one step',\n",
       " 'corresponds to one layer. In neural ODEs, this transformation is performed',\n",
       " \"continously. Now we don't have a discrete set of layers, enumerated by natural\",\n",
       " 'numbers $i=1, \\\\ldots, N$. Instead, we have a continuum set of “moments of time”,',\n",
       " 'represented as a segment $[0, T]$.  At each moment, we specify “infinitesimal',\n",
       " 'transformation” that occurs when the value passes through this moment.',\n",
       " '',\n",
       " 'Technically, neural ODEs are obtained as a limit case of so-called _residual',\n",
       " \"networks_ (also known as _ResNets_). In the residual networks, the output value of $i$'th layer is determined as\",\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " 'h_{i}=h_{i-1} + f^{i}_\\\\theta(h_{i-1}).',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'The difference with the usual neural networks is the presence of “$h_{i-1}+{}$” term.',\n",
       " 'It allows the network to learn more efficiently: ResNets can be very deep and',\n",
       " 'still learnable. (Note that to write such an equation we must demand that the',\n",
       " 'dimensionality of each layer be the same and equal to the dimensionality of the',\n",
       " 'input space.) Now we can imagine a very-very deep network: to make sure that',\n",
       " \"the output doesn't tend to infinity, let's add some small coefficient that will\",\n",
       " 'decrease as the network depth increases:',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " 'h_{i}=h_{i-1} + \\\\varepsilon f^{i}_\\\\theta(h_{i-1}), \\\\quad i=1,\\\\ldots, N,',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " \"where $\\\\varepsilon \\\\sim 1/N$. And that's an equation of well-known [Euler\",\n",
       " 'method](https://en.wikipedia.org/wiki/Euler_method) of the numerical solution of',\n",
       " 'a differential equation! As $N$ tends to infinity, the sequence of values $h_i$',\n",
       " \"tends to a solution of the corresponding differential equation. That's the\",\n",
       " 'rationale.',\n",
       " '',\n",
       " 'Now the formal settings. Consider a differential equation',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\dot x(t) = f_\\\\theta(t, x(t)),',\n",
       " '$${{< /math >}}',\n",
       " '',\n",
       " 'where $x$ is a function, $x(t) \\\\in \\\\mathbb R^n$, $n$ is the dimensionality of the input space, $\\\\dot x(t)$ is a derivative $dx(t)/dt$, $f_\\\\theta$ is a smooth',\n",
       " 'function that depends on the vector of parameters $\\\\theta$. Denote the input of',\n",
       " 'the network by $x_{input} \\\\in \\\\mathbb R^n.$ Consider a point $(t=0,',\n",
       " \"x=x_{input})$: that's our starting point. We can find a solution of the\",\n",
       " \"differential equation whose graph passes through this point. Let's denote this\",\n",
       " 'solution by $\\\\varphi(t; x_{input}; \\\\theta)$. By construction, $\\\\varphi(0;',\n",
       " 'x_{input}; \\\\theta)=x_{input}$.  The output of the network, by definition, is the value',\n",
       " 'of this solution at moment $T$, where $T$ is some fixed positive number. If one',\n",
       " 'changes the initial value $x_{input}$, the solution changes as well and so does',\n",
       " 'the output. In other words, our network defines a map',\n",
       " '',\n",
       " '{{< math >}}$$',\n",
       " '\\\\begin{align*}',\n",
       " ...]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "supposed-denial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line='---', mode='start'\n",
      "line='title: Adjoint State Method, Backpropagation and Neural ODEs', mode='preamble'\n",
      "line='', mode='preamble'\n",
      "line='summary: |-', mode='preamble'\n",
      "line='    You probably heard about Neural ODEs, a neural network architecture based on', mode='preamble'\n",
      "line='    the ordinary differential equations. To train this kind of models, a mysterious', mode='preamble'\n",
      "line='    trick called _adjoint state method_ is used. How does it work, why do we', mode='preamble'\n",
      "line='    need it and how it is related to backpropagation? ', mode='preamble'\n",
      "line='', mode='preamble'\n",
      "line='tags:', mode='preamble'\n",
      "line='- machine learning', mode='preamble'\n",
      "line='- differential equations', mode='preamble'\n",
      "line='- NeuralODE', mode='preamble'\n",
      "line='', mode='preamble'\n",
      "line='date: \"2022-08-05T21:58:00Z\"', mode='preamble'\n",
      "line='', mode='preamble'\n",
      "line='draft: false', mode='preamble'\n",
      "line='', mode='preamble'\n",
      "line='# Optional external URL for project (replaces project detail page).', mode='preamble'\n",
      "line='external_link: \"\"', mode='preamble'\n",
      "line='', mode='preamble'\n",
      "line='# image:', mode='preamble'\n",
      "line='#   caption: Photo by Michael Daniel ', mode='preamble'\n",
      "line='#   focal_point: Smart', mode='preamble'\n",
      "line='# url_code: \"\"', mode='preamble'\n",
      "line='url_pdf: \"\"', mode='preamble'\n",
      "line='url_slides: \"\"', mode='preamble'\n",
      "line='url_video: \"\"', mode='preamble'\n",
      "line='', mode='preamble'\n",
      "line='# Slides (optional).', mode='preamble'\n",
      "line='#   Associate this project with Markdown slides.', mode='preamble'\n",
      "line=\"#   Simply enter your slide deck's filename without extension.\", mode='preamble'\n",
      "line='#   E.g. `slides = \"example-slides\"` references `content/slides/example-slides.md`.', mode='preamble'\n",
      "line='#   Otherwise, set `slides = \"\"`.', mode='preamble'\n",
      "line='---', mode='preamble'\n",
      "line='', mode='normal'\n",
      "line='You probably heard about Neural ODEs[^1], a neural network architecture based on', mode='normal'\n",
      "line='the ordinary differential equations. When I first read about them, my thoughts were:', mode='normal'\n",
      "line='okay, we have an ODE given by a neural network, now we need to learn weights of', mode='normal'\n",
      "line='that neural network, thus we need gradients, thus we have to find derivates of', mode='normal'\n",
      "line='the solution of an ODE with respect to parameters. And we have a standard tool', mode='normal'\n",
      "line=\"for that in ODE theory, it's called “variational equations”, so let's just\", mode='normal'\n",
      "line=\"apply them and we're done. Case closed.\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='However, in the paper, the authors used much more elaborate approach, based', mode='normal'\n",
      "line='on something called _adjoint state method_. It was strange: why we need some cryptic', mode='normal'\n",
      "line='mathematical magic to solve such a standard problem? In various other', mode='normal'\n",
      "line='discussions, I heard from time to time that these mysterious _adjoints_', mode='normal'\n",
      "line='are somehow related to backpropagation. However, all tutorials on adjoint state', mode='normal'\n",
      "line='method I was able to find used a bunch of sophisticated infinite-dimensional', mode='normal'\n",
      "line='optimization theory, and it was not clear for me, how this can be related', mode='normal'\n",
      "line='to such a simple thing as backpropagation? ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='It was my surprise when I understood that adjoint state method in fact is based', mode='normal'\n",
      "line='on a very simple idea. And now I want share it with you.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='This post turned out to be rather long. I tried to make it as accessible as', mode='normal'\n",
      "line='possible, so included detailed explanations of all the derivations. This led', mode='normal'\n",
      "line=\"to inclusion of various equations that sometimes can look scary. Don't be afraid:\", mode='normal'\n",
      "line='there are also a lot of illustrations and informal descriptions to guide you', mode='normal'\n",
      "line='through the story.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='- In the [first part](#forward-and-backward), I recall briefly some', mode='normal'\n",
      "line='    notions from multidimensional calculus and present the main constructions', mode='normal'\n",
      "line='    that will be used later. The reader is expected to be familiar with matrix', mode='normal'\n",
      "line='    calculations, the notion of a derivative of multidimensional map and the', mode='normal'\n",
      "line='    chain rule, whilst the two latter will be recalled.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='- The [second part](#backpropagation-in-neural-networks)', mode='normal'\n",
      "line='    is an introduction to the backpropagation in the usual dense neural networks. Here I', mode='normal'\n",
      "line='    present an exposition that is focused on the effective implementation of the', mode='normal'\n",
      "line='    backpropagation using matrix calculations and also has close ties with the', mode='normal'\n",
      "line='    adjoint state method in the neural ODEs. Nevertheless, I hope you will find', mode='normal'\n",
      "line='    something new and interesting about backpropagation from this part even if you', mode='normal'\n",
      "line='    are not interested in the neural ODEs. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='- The [last part](#adjoint-state-method-in-neural-odes) is devoted to the', mode='normal'\n",
      "line='    adjoint state method. Here I expect some very basic knowledge of the ordinary', mode='normal'\n",
      "line='    differential equations. The main results from the ODE theory will be recalled.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='That is not an easy journey, but I hope you will find it as exciting as I did.', mode='normal'\n",
      "line='If you have any questions regarding this post, do not hesitate to get in touch', mode='normal'\n",
      "line='on', mode='normal'\n",
      "line='[Twitter](https://twitter.com/ilya_schurov).', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Now, let's go!\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='## Forward and backward', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### How to mupliply matrices', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Before we begin with backpropagation and neural ODEs, let's talk about something\", mode='normal'\n",
      "line='very simple: about matrix multiplication.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Assume we have two square {{< math >}}$n \\\\times n${{< /math >}} matrices, {{< math >}}$A${{< /math >}} and {{< math >}}$B${{<', mode='normal'\n",
      "line='/math >}}, and {{< math >}}$n${{< /math >}}-dimensional vector (vector-column) {{< math >}}$x${{< /math >}}. Consider the', mode='normal'\n",
      "line='following product:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$ABx$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line=\"As matrix multiplication is associatative, we don't need any brackets in this\", mode='normal'\n",
      "line=\"formula. However, if we try to put them, we'll note that it can be done in two\", mode='normal'\n",
      "line='different ways: we can either write it like this:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$(AB)x$$', mode='math'\n",
      "line='{{< /math >}} ', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='or like this:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$A(Bx).$$', mode='math'\n",
      "line='{{< /math >}} ', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line=\"Of course, we'll get the same results, but computationally these two formulas\", mode='normal'\n",
      "line='are different. In the first case, we find matrix {{< math >}}$AB${{< /math >}},', mode='normal'\n",
      "line='that takes {{< math >}}$O(n^3)${{< /math >}} elementary multiplications, then keep', mode='normal'\n",
      "line='this new matrix in memory, that is {{< math >}}$O(n^2)${{< /math >}}, and then', mode='normal'\n",
      "line='multiply it on {{< math >}}$x${{< /math >}}. The last operation is cheep and', mode='normal'\n",
      "line='only needs {{< math >}}$O(n^2)${{< /math >}} operations.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='In the second approach, we first find {{< math >}}$Bx${{< /math >}}, that is', mode='normal'\n",
      "line='cheap, {{< math >}}$O(n^2)${{< /math >}} operations and {{< math >}}$O(n)${{<', mode='normal'\n",
      "line='/math >}} memory. Than we multiply {{< math >}}$A${{< /math >}} by the result of', mode='normal'\n",
      "line=\"previous computation, that is again cheap. And we're done! So, the difference\", mode='normal'\n",
      "line='between two method is dramatic: {{< math >}}$O(n^3)${{< /math >}} vs. {{< math', mode='normal'\n",
      "line='>}}$O(n^2)${{< /math >}} in operations and {{< math >}}$O(n^2)${{< /math >}} vs.', mode='normal'\n",
      "line='{{< math >}}$O(n)${{< /math >}} in memory. The second approach is much more', mode='normal'\n",
      "line='efficient!', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Of course, it works only if we have only one vector {{< math >}}$x${{< /math >}} that should be multiplied by {{< math >}}$AB${{< /math >}}; if there are', mode='normal'\n",
      "line='many such vectors, it can be more efficient to find the product {{< math >}}$AB${{< /math >}} once and then reuse it. However, as we will see below, in', mode='normal'\n",
      "line='our problems, including backpropagation, this kind of calculation is effectively', mode='normal'\n",
      "line='one-time. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The last thing I want to mention here is that if instead of vector-column $x$ we', mode='normal'\n",
      "line='consider vector-row {{< math >}}$u${{< /math >}} (mathematically speaking, rather ', mode='normal'\n",
      "line='covector than vector, if we represent vectors as vector-columns), and we want to', mode='normal'\n",
      "line='find a product {{< math >}}$uAB${{< /math >}}, this again can be done in two', mode='normal'\n",
      "line='different ways:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$', mode='math'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{phiAB}', mode='math'\n",
      "line='uAB=(uA)B=u (AB),', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='and now the first one is much cheaper.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Does it sounds reasonable? If yes, congratulations: you understood the main idea', mode='normal'\n",
      "line='of the adjoint state method!', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Derivatives and gradients', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='In what follows, we will be interested in maps from multidimensional spaces to', mode='normal'\n",
      "line='multiminesional spaces (i.e. from $\\\\mathbb R^n$ to $\\\\mathbb R^m$ for some', mode='normal'\n",
      "line='positive integer $n$ and $m$) and their derivatives. In general, we treat a', mode='normal'\n",
      "line='derivative of a map $f\\\\colon \\\\mathbb R^n \\\\to \\\\mathbb R^m$ as the [Jacobian matrix](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant), i.e.', mode='normal'\n",
      "line='matrix of partial derivatives of components of $f$. We will denote it', mode='normal'\n",
      "line='by $\\\\partial f(x)/\\\\partial x$, sometimes ommitting $(x)$. This matrix has $n$', mode='normal'\n",
      "line='columns and $m$ rows. By definition of a derivative, for any vector $\\\\Delta x \\\\in \\\\mathbb R^n$', mode='normal'\n",
      "line='from some neighborhood of $0$,', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$', mode='math'\n",
      "line='f(x+\\\\Delta x)-f(x)=\\\\frac{\\\\partial f(x)}{\\\\partial x} \\\\Delta x+o(\\\\|v\\\\|),', mode='math'\n",
      "line='$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='or, informally,', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$ f(x+\\\\Delta x)-f(x) \\\\approx \\\\frac{\\\\partial f(x)}{\\\\partial x} \\\\Delta x.', mode='math'\n",
      "line='$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='A useful illustration of this approximation is given on [Figure 1](#figure-ill-deriv): for map from', mode='normal'\n",
      "line='$\\\\mathbb R$ to $\\\\mathbb R$, the length of an image of the small segment is', mode='normal'\n",
      "line='approximately equal to the derivative multiplied by the length of the segment', mode='normal'\n",
      "line='itself. If we replace segments with vectors based at $x$, the same illustration', mode='normal'\n",
      "line='will work for multidimensional case.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-14.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 1. Illustration of a derivative\" id=\"ill-deriv\" >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='In a special case when $m=1$, the derivative is a matrix with 1 row, i.e. it is', mode='normal'\n",
      "line='a vector-row. In this case we also call it a *gradient* of $f$ and denote by', mode='normal'\n",
      "line='{{< math >}}$\\\\nabla_{\\\\!x} f(x)${{< /math >}}. (Strictly speaking, one should', mode='normal'\n",
      "line='call this vector-row differential, not gradient, because it is a covector, and', mode='normal'\n",
      "line=\"gradient is a vector, but we'll discuss the\", mode='normal'\n",
      "line='difference between them next time.)', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Let us also recall the well-known ', mode='normal'\n",
      "line='[chain rule](https://en.wikipedia.org/wiki/Chain_rule)', mode='normal'\n",
      "line='that simply says that derivative of a composition $g\\\\circ f$ is a product', mode='normal'\n",
      "line='(i.e. composition) of the derivatives:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{chain-rule}', mode='math'\n",
      "line='\\\\frac{\\\\partial (g\\\\circ f(x))}{\\\\partial x} = \\\\frac{\\\\partial g(h)}{\\\\partial', mode='math'\n",
      "line='h}\\\\frac{\\\\partial f(x)}{\\\\partial x},', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where the first derivative is taken at point $h=f(x)$.', mode='normal'\n",
      "line='This formula can be easily', mode='normal'\n",
      "line='illustrated with the following picture.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-15.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 2. The chain rule\" id=\"chainrule\" >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='We will also consider functions that depend on additional multidimensional', mode='normal'\n",
      "line='parameter, usually denoted by $\\\\theta \\\\in \\\\mathbb R^p$. Formally, such a', mode='normal'\n",
      "line='function is just a map', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='f\\\\colon \\\\mathbb R^n \\\\times \\\\mathbb R^p \\\\to \\\\mathbb R^m,', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='i.e. it is a function of two vector arguments $f(x, \\\\theta)$, but we usually write it like $f_\\\\theta(x)$ instead. In this case, $\\\\partial f_\\\\theta/\\\\partial x$ is a derivative with', mode='normal'\n",
      "line='respect to argument $x$ (keeping $\\\\theta$ fixed) and $\\\\partial f_\\\\theta / \\\\partial', mode='normal'\n",
      "line='\\\\theta$ is a derivative with respect to parameter (keeping $x$ fixed). The', mode='normal'\n",
      "line='following approximations take place:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{gather}', mode='math'\n",
      "line='\\\\label{x-approx}', mode='math'\n",
      "line='f_\\\\theta(x+\\\\Delta x)-f_\\\\theta(x) \\\\approx \\\\frac{\\\\partial f_\\\\theta(x)}{\\\\partial x} \\\\Delta x,\\\\\\\\', mode='math'\n",
      "line='\\\\label{theta-approx}', mode='math'\n",
      "line='f_{\\\\theta+\\\\Delta \\\\theta}(x)-f_\\\\theta(x) \\\\approx \\\\frac{\\\\partial f_\\\\theta(x)}{\\\\partial \\\\theta} \\\\Delta \\\\theta.\\\\\\\\', mode='math'\n",
      "line='\\\\end{gather}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='### Gradient of composition', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Now let us consider a map $G\\\\colon \\\\mathbb R^n \\\\to \\\\mathbb R$ that can be', mode='normal'\n",
      "line='represented as a composition:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$G(x)=g^N\\\\circ g^{N-1} \\\\circ\\\\cdots \\\\circ g^1(x),$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where $g^1, \\\\ldots, g^N$ are some differentiable maps from multidimensional spaces', mode='normal'\n",
      "line='to multidimensional spaces. (Superscripts do not denote powers here.) ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='It is very important for us that codomain of $G$ is one-dimensional. When we', mode='normal'\n",
      "line='will discuss neural networks, $G$ will represent some loss function with values', mode='normal'\n",
      "line='in $\\\\mathbb R$. As $g^N$ is the last applied map, its codomain coincides with', mode='normal'\n",
      "line='the codomain of $G$ and thus it is one-dimensional as well.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-5.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 3. Composition of several functions. All axes except the last one represent multidimensional spaces. The last axis is one-dimensional\" id=\"composition\" >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='If we have a value $x$ and want to find $G(x)$, the algorithm is', mode='normal'\n",
      "line='straightforward: we find $h_1:=g^1(x)$, put it into $g^2$, thus finding', mode='normal'\n",
      "line='$h_2:=g^2(h_1)$, put it into $g^3$ and so on, the last step is $y=g^N(h_{N-1})$.', mode='normal'\n",
      "line='The flow of calculation is forward, from smaller indexes to larger', mode='normal'\n",
      "line='(right-to-left, if we look at the formula, or left-to-right, if we look at the', mode='normal'\n",
      "line='picture).  This is what usually called', mode='normal'\n",
      "line='“forward pass” in the neural networks.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Now what if we want to find the gradient {{< math >}}$\\\\nabla_{\\\\! x}$', mode='normal'\n",
      "line='{{< /math >}} (or, in other terms, the derivative $\\\\partial G / \\\\partial x$)?', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The chain rule \\\\eqref{chain-rule} immediately gives us:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$', mode='math'\n",
      "line='\\\\nabla_{\\\\! x} G=', mode='math'\n",
      "line='\\\\frac{\\\\partial g^N}{\\\\partial h_{N-1}}', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{N-1}}{\\\\partial h_{N-2}}\\\\cdots', mode='math'\n",
      "line='\\\\frac{\\\\partial g^2}{\\\\partial h_1} \\\\frac{\\\\partial g^1}{\\\\partial x}.', mode='math'\n",
      "line='$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='As we already said, $g^N$ maps to $\\\\mathbb R$, and therefore one can denote the first multiplier in this product by {{< math >}}$\\\\nabla_{\\\\!h_{N-1}} g^N${{< /math >}}:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$', mode='math'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{nablaG}', mode='math'\n",
      "line='\\\\nabla_{\\\\! x} G=', mode='math'\n",
      "line='\\\\nabla_{\\\\!h_{N-1}} g^N', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{N-1}}{\\\\partial h_{N-2}}\\\\cdots', mode='math'\n",
      "line='\\\\frac{\\\\partial g^2}{\\\\partial h_1} \\\\frac{\\\\partial g^1}{\\\\partial x}.', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='How to use this equation to find the gradient? First of all, we have to find', mode='normal'\n",
      "line='$h_1, \\\\ldots, h_{N-1}$, i.e. perform all the steps of the forward pass (except', mode='normal'\n",
      "line='the last one). Then we have to find all the derivatives of functions', mode='normal'\n",
      "line='$g^N$, $g^{N-1}$, …, $g^2$, $g^1$ at the corresponding points $h_{N-1}$, $h_{N-2}$,', mode='normal'\n",
      "line='…, $h_1$, $x$.', mode='normal'\n",
      "line='Then we have to multiply everything.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='As the leftmost multiplier is a vector-row, we are in the situation very similar', mode='normal'\n",
      "line='to equation', mode='normal'\n",
      "line='$\\\\eqref{phiAB}$: we have a vector-row that is multiplied to a product of', mode='normal'\n",
      "line='matrices. Just like we discussed above, the most natural and efficient way ', mode='normal'\n",
      "line='is to do it left-to-right: we first find a product', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\nabla_{\\\\!h_{N-1}} g^N', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{N-1}}{\\\\partial h_{N-2}},', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='obtain a new vector-row, multiply it by the next matrix, and so on. Now the', mode='normal'\n",
      "line='calculation flow goes backward, from the terms with large indexes to the terms with', mode='normal'\n",
      "line='small indexes (left-to-right if we look at the formula, or right-to-left if we', mode='normal'\n",
      "line='look at the picture). This is what is known as _backward pass_.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Theoretically, one _can_ find the product in the right-hand side of equation', mode='normal'\n",
      "line='$\\\\eqref{nablaG}$ in a different order, e.g. right-to-left, but it would  not be', mode='normal'\n",
      "line='very efficient: one had to find and store some large intermediate matrices', mode='normal'\n",
      "line='during the calculations. In our approach, we store only the initial matrices and', mode='normal'\n",
      "line='intermediate vector-rows.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='To summarise: consider a function that is given as a', mode='normal'\n",
      "line='composition. There are two natural problems associated with it: to find its', mode='normal'\n",
      "line='value at a particular point and to find its gradient. The flow of calculation', mode='normal'\n",
      "line='of the value is forward, and the flow of calculation of the gradient is', mode='normal'\n",
      "line='backward. To perform backward pass, we need to perform forward pass first to be', mode='normal'\n",
      "line='able to find the derivatives that are needed in the backward pass.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Truncated compositions', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='It is instructive to study intermediate steps of the forward and the backward passes.', mode='normal'\n",
      "line=\"Let's begin with the forward pass. \", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='For each', mode='normal'\n",
      "line='integer $j$, $0 < j \\\\le N$, ', mode='normal'\n",
      "line='consider the following “truncated” composition:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='G^{0:j}(x):=g^j \\\\circ g^{j-1}\\\\circ \\\\cdots \\\\circ g^{1}(x).', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-6.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 4. Forward truncated compositions\" id=\"forward-trunc\"', mode='normal'\n",
      "line='>}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Each $G^{0:j}$ shows how $h_j$ depends on $x$. In the forward pass, we find', mode='normal'\n",
      "line='consequently $G^{0:1}(x)$, $G^{0:2}(x)$, and so on.  At step $j$ we find', mode='normal'\n",
      "line='$G^{0:j}(x)$ applying $g^j$ to the result of the previous step. At the last step', mode='normal'\n",
      "line=\"$N$ we find $G^{0:N}(x)=G(x)$. That's literally straightforward.\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='To consider backward pass, we need a different “truncation”. For each', mode='normal'\n",
      "line='integer $i$, $0 \\\\le i < N$, let', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{truncated}', mode='math'\n",
      "line='G^{i:N}(h_i):=g^N \\\\circ g^{N-1}\\\\circ \\\\cdots \\\\circ g^{i+1}(h_i).', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-7.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 5. Backward truncated compositions\"', mode='normal'\n",
      "line='id=\"back-trunc\"', mode='normal'\n",
      "line='>}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='It is a map with codomain $\\\\mathbb R$ that shows how $y$ depends on $h_i$. Its gradient can be found using the chain rule:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\nabla_{\\\\! h_{i}} G^{i:N}=', mode='math'\n",
      "line='\\\\nabla_{\\\\!h_{N-1}} g^N', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{N-1}}{\\\\partial h_{N-2}}\\\\cdots', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{i+1}}{\\\\partial h_{i}}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='One can see that the right-hand side of this equation is a truncated version of equation $\\\\eqref{nablaG}$: we', mode='normal'\n",
      "line='only keep the first $(N-i)$ multipliers. And this is exactly what backward pass', mode='normal'\n",
      "line='calculates at each step: for each $i$ decreasing from $(N-1)$ to $0$, we find {{< math >}}$ \\\\nabla_{\\\\! h_{i}} G^{i:N}${{< /math >}}', mode='normal'\n",
      "line='multiplying the result of the previous step to $\\\\partial g^{i+1}/\\\\partial h_i$:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{nablastep}', mode='math'\n",
      "line='\\\\nabla_{\\\\! h_{i}} G^{i:N}=\\\\nabla_{\\\\! h_{i+1}}G^{i+1:N}\\\\cdot \\\\frac{\\\\partial g^{i+1}}{\\\\partial h_{i}}.', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Clearly, $G^{0:N}=G$ and at the last step we obtain gradient of $G$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Here we see that forward and backward passes are very similar in nature, but at', mode='normal'\n",
      "line='the same time has substantial difference. In the forward pass, the domain of', mode='normal'\n",
      "line=\"each function $G^{0:j}$ we consider is fixed (it's $\\\\mathbb R^n$, the same as the\", mode='normal'\n",
      "line='domain of $G$), but codomain shifts in “forward” direction, see ', mode='normal'\n",
      "line='[Figure 4](#figure-forward-trunc). In the backward', mode='normal'\n",
      "line=\"pass, the codomain of the function $G^{i:N}$ is fixed (it's $\\\\mathbb R$, the\", mode='normal'\n",
      "line='same as the codomain of $G$), but domain shifts in “backward” direction: at step', mode='normal'\n",
      "line='$i$, argument of $G^{i:N}$ is $h_i$, and $i$ decreases, see [Figure', mode='normal'\n",
      "line='5](#figure-back-trunc).', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Let us summarise with an informal description. During the calculations, we want', mode='normal'\n",
      "line='to begin with a simple', mode='normal'\n",
      "line='object and transform it to the object we need. In the forward pass, the simple', mode='normal'\n",
      "line='object is just a vector $x$, that “lives” at the “beginning” of the composition.', mode='normal'\n",
      "line=\"We transform it by application of the corresponding $g^j$'s until we pull it\", mode='normal'\n",
      "line='through the whole composition and get $G(x)$. In the backward pass, the simple', mode='normal'\n",
      "line='object we begin with is the gradient $\\\\nabla_{\\\\\\\\! h_{N-1}} g^{N}$. We can be', mode='normal'\n",
      "line=\"sure it's “simple” (i.e. a vector-row, not a full matrix) because $g^N$'s\", mode='normal'\n",
      "line='codomain is guaranteed to be one-dimensional. This gradient “lives“ at the “end”', mode='normal'\n",
      "line='of the composition, and it is natural to transform it by extending “backward”.', mode='normal'\n",
      "line='When we pull it through the whole composition, we get the desired gradient', mode='normal'\n",
      "line='$\\\\nabla_{\\\\\\\\! x} G$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< spoiler', mode='normal'\n",
      "line='text=\"Interested in mathematical details? Click here!\" >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='I cannot resist the temptation to discuss a bit more mathematical perspective on', mode='normal'\n",
      "line='equation \\\\eqref{nablastep} and add some rigour to the informal description above. ', mode='normal'\n",
      "line='To this end, we have to define formally the spaces', mode='normal'\n",
      "line='where the gradients live.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Let's say that for each $i=1,\\\\ldots, N$,  $g^i$ is a map from $\\\\mathcal\", mode='normal'\n",
      "line='M_{i-1}=\\\\mathbb R^{n_{i-1}}$ to $\\\\mathcal M_{i}=\\\\mathbb R^{n_i}$, $n_N=1$. As', mode='normal'\n",
      "line='before, $h_i = g^i(h_{i-1})$ and $h_0=x$. The gradient $\\\\nabla_{\\\\\\\\! h_i}', mode='normal'\n",
      "line='G^{i:N}$ is a linear map that acts on vectors $\\\\Delta h_i$. ', mode='normal'\n",
      "line='It is natural to think about this vectors as based at point', mode='normal'\n",
      "line='$h_i$. The vector space of all such vectors is called a _tangent space_ of ', mode='normal'\n",
      "line='$\\\\mathcal M_{i}$ at point $h_i$; it is denoted by $T_{h_i} \\\\mathcal M_i$. Thus the', mode='normal'\n",
      "line='gradient $\\\\nabla_{\\\\\\\\! h_i} G^{i:N}$ is a linear map from $T_{h_i} \\\\mathcal M_i$', mode='normal'\n",
      "line='to $\\\\mathbb R$, such linear maps (with codomain $\\\\mathbb R$) also known as', mode='normal'\n",
      "line='_linear functionals_ or _covectors_. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The set of all linear functionals defined on some vector space $V$ is again a', mode='normal'\n",
      "line='vector space: one can add linear functionals to each other and multiply them by', mode='normal'\n",
      "line='real numbers. This space is called _dual space_ to $V$ and denoted by $V^*$. The', mode='normal'\n",
      "line=\"dual space to the tangent space $T_{h_i} \\\\mathcal M_i$ has a special name: it's\", mode='normal'\n",
      "line='called a _cotangent space_ of $\\\\mathcal M_i$ at point $h_i$ and denoted by', mode='normal'\n",
      "line='$T_{h_i}^\\\\* \\\\mathcal M_i$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='So, the gradient $\\\\nabla_{\\\\\\\\! h_i} G^{i:N}$ belongs to the cotangent space', mode='normal'\n",
      "line='$T_{h_i}^\\\\* \\\\mathcal M_i$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Now let's consider a derivative of $g^i$ at point $h_{i-1}$.\", mode='normal'\n",
      "line='It is a linear map that transforms vectors based at point $h_{i-1}$ to vectors', mode='normal'\n",
      "line=\"based at point $h_i$, so it's a map\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\frac{\\\\partial g^i(h_{i-1})}{\\\\partial h_{i-1}}\\\\colon T_{h_{i-1}} \\\\mathcal M_{i-1}', mode='math'\n",
      "line='\\\\to T_{h_i} \\\\mathcal M_i.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Now I want to consider a very abstract setup that distills the main relations', mode='normal'\n",
      "line='between the objects we introduced so far. We have two vector spaces, denote them', mode='normal'\n",
      "line='by $V$ and $W$, and a linear map ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\mathcal A\\\\colon V \\\\to W.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Consider the dual spaces $V^\\\\*$ and $W^\\\\*$. Then $\\\\mathcal A$ naturally induces', mode='normal'\n",
      "line='a map ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$\\\\mathcal A^\\\\*\\\\colon W^\\\\* \\\\to V^\\\\*$$ ', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='(Compare this equation with the equation above. You see:', mode='normal'\n",
      "line='$V$ and $W$ are swapped!) For each covector $\\\\psi \\\\in V^*$, we define its image', mode='normal'\n",
      "line='$\\\\mathcal A^\\\\* \\\\psi$ with the following formula:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='(\\\\mathcal A^\\\\* \\\\psi)(v)=\\\\psi (\\\\mathcal A v)\\\\quad \\\\text{for each $v\\\\in V$.}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='What is written here? First, as $\\\\mathcal A^\\\\*$ acts from $W^\\\\*$ to $V^\\\\*$, the', mode='normal'\n",
      "line='image $\\\\mathcal A^\\\\* \\\\psi$ is a covector in $V^\\\\*$, i.e. it is a', mode='normal'\n",
      "line='linear functional defined on $V$. To define this functional, we have to define how', mode='normal'\n",
      "line='it acts on vectors. The value of $\\\\mathcal A^* \\\\psi$ on a vector $v \\\\in V$ is defined', mode='normal'\n",
      "line='in the following way: first, we apply operator $\\\\mathcal A$ to $v$, get a new', mode='normal'\n",
      "line='vector that belongs to $W$, then apply functional $\\\\psi$ (that works on $W$)', mode='normal'\n",
      "line='to this vector. The result is the value of the functional $\\\\mathcal A^\\\\* \\\\psi$', mode='normal'\n",
      "line='on the vector $v$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Operator $\\\\mathcal A^\\\\*$ is called an _adjoint_ to $\\\\mathcal A$. If you think about', mode='normal'\n",
      "line='it a little bit, you see that this construction is very-very natural. In fact,', mode='normal'\n",
      "line='it is an example of ', mode='normal'\n",
      "line='[contravariant Hom-functor](https://en.wikipedia.org/wiki/Hom_functor) in category theory, but we will not dive into such depths.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Let's return to our derivatives. Now we can consider an adjoint to the\", mode='normal'\n",
      "line='derivative $\\\\partial g^i / \\\\partial h_{i-1}$:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\left(\\\\frac{\\\\partial g^i(h\\\\_{i-1})}{\\\\partial h\\\\_{i-1}}\\\\right)^\\\\* \\\\colon T^\\\\*\\\\_{h_{i}} \\\\mathcal M\\\\_{i}', mode='math'\n",
      "line='\\\\to T^*\\\\_{h\\\\_{i-1}} \\\\mathcal M\\\\_{i-1}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='And equation \\\\eqref{nablastep} takes form:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\nabla_{\\\\\\\\\\\\! h_{i}} G^{i:N}=\\\\left(\\\\frac{\\\\partial g^{i+1}}{\\\\partial h_{i}}\\\\right)^\\\\*', mode='math'\n",
      "line='\\\\nabla_{\\\\\\\\\\\\! h_{i+1}}G^{i+1:N}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='(Just check from the definition of adjoint that this is indeed equivalent to', mode='normal'\n",
      "line='\\\\eqref{nablastep}.)', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='So, it is the adjoint to the derivative of $g^i$ that acts on the gradients! And as it is', mode='normal'\n",
      "line='an adjoint, it acts “backwards” relative to the action of the derivative itself', mode='normal'\n",
      "line='(and thus to the map $g^i$). So it solves the mystery of “backwardness” in', mode='normal'\n",
      "line='backpropagation.', mode='normal'\n",
      "line='Mathematically speaking, we are simply applying contravariant Hom-functor and it', mode='normal'\n",
      "line=\"reverses all the arrows. That's it!\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /spoiler >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Now let's look how it works in the neural networks.\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='## Backpropagation in neural networks', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The backpropagation algorithm is very much well-known, but I', mode='normal'\n",
      "line='present here an exposition that is specifically designed to stress the relation', mode='normal'\n",
      "line='of the backprop and the adjoint state method in the  neural ODEs.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### The usual neural network', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='For simplicity, assume we have a neural network that consists only of three', mode='normal'\n",
      "line='layers, two of them are hidden. Layer number {{< math >}}$i${{< /math >}}, ', mode='normal'\n",
      "line='{{< math >}}$i=1,2,3${{< /math >}}, transforms its input to output using a', mode='normal'\n",
      "line='function', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='    f^{i}_\\\\theta\\\\colon \\\\mathbb R^{n_{i-1}} \\\\to \\\\mathbb R^{n_i},', mode='math'\n",
      "line='$${{< /math >}} ', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where {{< math >}}$\\\\theta\\\\in \\\\mathbb R^p${{< /math >}}', mode='normal'\n",
      "line='is a vector of all parameters of the neural network (i.e. all weights and', mode='normal'\n",
      "line='biases), {{< math >}}$n_i${{< /math >}} is the dimensionality of the output of', mode='normal'\n",
      "line=\"{{< math >}}$i${{< /math >}}'th layer, {{< math >}}$n_0${{< /math >}} is the input\", mode='normal'\n",
      "line='dimensionality of the network. Usually each layer depends only', mode='normal'\n",
      "line='on a subset of parameters in {{< math >}}$\\\\theta${{< /math >}} and implements an', mode='normal'\n",
      "line='affine function in elementwise composition with nonlinear activation function,', mode='normal'\n",
      "line='but we are not interested in such architecture details now and consider rather', mode='normal'\n",
      "line='general case. The full network', mode='normal'\n",
      "line='defines a function', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='    f_{\\\\theta}(x) := f^{3}_\\\\theta\\\\circ f^{2}_\\\\theta \\\\circ f^{1}_\\\\theta(x)', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='This is a very similar to that discussed in the [previous', mode='normal'\n",
      "line='section](#gradient-of-composition). The main difference is that now all the', mode='normal'\n",
      "line='functions in this composition depend also on the parameter $\\\\theta$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Our composition can be visualized in the following way:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-1.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 6. Three-layer neural network\"', mode='normal'\n",
      "line='id=\"three-layer\"', mode='normal'\n",
      "line='>}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='We also have some loss function $L(y, y_{true})$ (e.g. in case of quadratic', mode='normal'\n",
      "line='loss, $L(y, y_{true})=(y-y_{true})^2$). If we put the output of the network into', mode='normal'\n",
      "line='the loss, we obtain an optimization objective', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\mathcal L(\\\\theta) := L(f_\\\\theta(x_{input}), y_{true})', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='that should be minimized during the training.  For simplicity, we are discussing', mode='normal'\n",
      "line='the loss at one datapoint; in the real settings, we would average this over the', mode='normal'\n",
      "line='batch.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Gradient of the loss', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='To perform the optimization of $\\\\mathcal L(\\\\theta)$ with gradient descent, one need to find its gradient. Chain rule immediately gives:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{nablamathcalL}', mode='math'\n",
      "line='\\\\nabla_{\\\\!\\\\theta} \\\\mathcal L(\\\\theta) = \\\\nabla_{\\\\!y} L \\\\cdot \\\\frac{\\\\partial', mode='math'\n",
      "line='f_\\\\theta(x_{input})}{\\\\partial \\\\theta},', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where the first multiplier is a gradient of {{< math >}}$L${{< /math >}}, i.e.', mode='normal'\n",
      "line='vector-row of dimensionality {{< math >}}$n_3${{< /math >}} (dimensionality of', mode='normal'\n",
      "line='the output layer), and the second multiplier is a ', mode='normal'\n",
      "line='{{< math >}}$(n_3 \\\\times p)${{< /math >}}-matrix.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='It is easy to find {{< math >}}$\\\\nabla_{\\\\!y} L ${{< /math >}} provided that {{< math', mode='normal'\n",
      "line='>}}$y${{< /math >}} is already calculated (i.e. in the case of quadratic loss,', mode='normal'\n",
      "line=\"it's just {{< math >}}$(2y-2y_{true})${{< /math >}}). To find the second\", mode='normal'\n",
      "line='multiplier, one have to decompose {{< math >}}$f_\\\\theta${{< /math >}} into a', mode='normal'\n",
      "line='composition of subsequent layer maps and again apply the chain rule. In contrast', mode='normal'\n",
      "line='with the [previous part](#gradient-of-composition), each function now depends not only on its argument, but', mode='normal'\n",
      "line=\"also on the parameter $\\\\theta$. This leads to new phenomena and I'd like to\", mode='normal'\n",
      "line='study it with some not-so-rigorous visualization.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Let's fix some small vector {{< math >}}$\\\\Delta \\\\theta \\\\in \\\\mathbb R^p${{< /math >}} and consider a “trajectory” of $x_{input}$ under the action of the “perturbed” maps\", mode='normal'\n",
      "line='{{< math >}}$f^{i}_{\\\\theta+\\\\Delta \\\\theta}${{< /math >}}, {{< math >}}$i=1,2,3${{<', mode='normal'\n",
      "line='/math >}}:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-2.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 7. What happens with the output of neural network if we slighly change parameters.\"', mode='normal'\n",
      "line='id=\"nn-change\"', mode='normal'\n",
      "line='>}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The difference between outputs {{< math >}}$f_{\\\\theta+\\\\Delta', mode='normal'\n",
      "line='\\\\theta}(x_{input})-f_\\\\theta(x_{input})${{< /math >}} is approximately equal to', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\frac{\\\\partial f_\\\\theta}{\\\\partial \\\\theta} \\\\Delta \\\\theta', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='provided that {{< math >}}$\\\\Delta \\\\theta${{< /math >}} is small by the definition of the derivative, see equation \\\\eqref{theta-approx}. (Note that on', mode='normal'\n",
      "line='the picture this difference is represented by a segment on a line, but in', mode='normal'\n",
      "line=\"reality it's a {{< math >}}$n_3${{< /math >}}-dimensional vector.) \", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Derivative of the network', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Now let's decompose this difference into a sum of three parts in the following\", mode='normal'\n",
      "line='way (see [Figure 8](#figure-decomp-net) below). For each of the intermediate points of the unperturbed', mode='normal'\n",
      "line='trajectory (i.e. $f^1_\\\\theta(x_{input})$ and $f^2_\\\\theta\\\\circ f^1_\\\\theta(x_{input})$), we', mode='normal'\n",
      "line='consider a trajectory of the perturbed network that starts from this point.', mode='normal'\n",
      "line='These trajectories split the segment $[f_{\\\\theta}(x_{input}),', mode='normal'\n",
      "line='f_{\\\\theta+\\\\Delta \\\\theta}(x_{input})]$ into three smaller segments denoted (from top to bottom)', mode='normal'\n",
      "line='by {{< math >}}$\\\\Delta^3_1${{< /math >}}, {{< math >}}$\\\\Delta^3_2${{< /math >}}', mode='normal'\n",
      "line='and {{< math >}}$\\\\Delta^3_3${{< /math >}}.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-4.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 8. Decomposition of the network\\'s derivative\"', mode='normal'\n",
      "line='id=\"decomp-net\"', mode='normal'\n",
      "line='>}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Here all the red arrows represent the action of the corresponding {{< math', mode='normal'\n",
      "line='>}}$f^i_{\\\\theta+\\\\Delta \\\\theta}${{< /math >}}. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Of course, this is not an exact figure: in reality, the output space is', mode='normal'\n",
      "line='multidimensional, and we do not split a segment into smaller segments.', mode='normal'\n",
      "line='Nevertheless, the argument is correct: we can represent a vector from', mode='normal'\n",
      "line='$f_\\\\theta(x_{input})$ to $f_{\\\\theta+\\\\Delta \\\\theta}(x_{input})$ as a sum of three', mode='normal'\n",
      "line='vectors given as a difference between the values of the corresponding', mode='normal'\n",
      "line='compositions. So, no cheating here!', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='We will approximate each of the smaller parts using the appropriate derivatives.', mode='normal'\n",
      "line=\"Let's begin with {{< math >}}$\\\\Delta^3_3${{< /math >}}. It measures the\", mode='normal'\n",
      "line='difference between the images of some point under action of {{< math', mode='normal'\n",
      "line='>}}$f^3_{\\\\theta+\\\\Delta \\\\theta}${{< /math >}} and {{< math >}}$f^3_{\\\\theta}${{<', mode='normal'\n",
      "line='/math >}}. Again, we use the definition of a derivate (particularly, equation', mode='normal'\n",
      "line='\\\\eqref{theta-approx}) and get the following', mode='normal'\n",
      "line='approximation:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\Delta^3_3 \\\\approx \\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} \\\\Delta \\\\theta.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='That was easy. Now consider {{< math >}}$\\\\Delta^3_2${{< /math >}}. Here we have', mode='normal'\n",
      "line='two steps. At the first step, we have two functions, {{< math >}}$f^2_\\\\theta${{<', mode='normal'\n",
      "line='/math >}} and {{< math >}}$f^2_{\\\\theta+\\\\Delta \\\\theta}${{< /math >}} that are applied to the same point. The difference between the images is denoted by {{< math >}}$\\\\Delta^2_2${{< /math >}} and like in the previous case is approximately equal to ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\Delta^2_2 \\\\approx \\\\frac{\\\\partial f^2_{\\\\theta}}{\\\\partial \\\\theta} \\\\Delta \\\\theta.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='At the second step, we have ', mode='normal'\n",
      "line='one function, {{< math >}}$f^2_{\\\\theta+\\\\Delta \\\\theta}${{< /math >}}, that is', mode='normal'\n",
      "line='applied to two different points. To find the difference between the images now,', mode='normal'\n",
      "line='we have to use the derivative of {{< math >}}$f^3_{\\\\theta+\\\\Delta \\\\theta}(h_2)${{< /math >}} with', mode='normal'\n",
      "line='respect to its argument {{< math >}}$h_2${{< /math >}}, see equation', mode='normal'\n",
      "line='\\\\eqref{x-approx}. Namely:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\Delta^3_2 \\\\approx ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta + \\\\Delta \\\\theta}}{\\\\partial h_2}\\\\Delta^2_2 \\\\approx ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta}}{\\\\partial \\\\theta} \\\\Delta \\\\theta.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='And finally for {{< math >}}$\\\\Delta^3_1${{< /math >}} we have three steps:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\Delta^3_1 \\\\approx ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta + \\\\Delta \\\\theta}}{\\\\partial h_2}\\\\Delta^2_1 \\\\approx ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_1} \\\\Delta_1^1 \\\\approx', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial', mode='math'\n",
      "line='\\\\theta} \\\\Delta \\\\theta.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line=\"Now let's sum up everything:\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{align*}', mode='math'\n",
      "line='\\\\frac{\\\\partial f_{\\\\theta}}{\\\\partial \\\\theta}\\\\Delta \\\\theta \\\\approx {} & \\\\Delta^3_3 +', mode='math'\n",
      "line='\\\\Delta^3_2 + \\\\Delta^3_1 \\\\approx  \\\\\\\\', mode='math'\n",
      "line='& \\\\left(', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} + ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta}}{\\\\partial \\\\theta} +', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta+\\\\Delta \\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial', mode='math'\n",
      "line='\\\\theta}\\\\right) \\\\Delta \\\\theta.', mode='math'\n",
      "line='\\\\end{align*}', mode='math'\n",
      "line='$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='As $\\\\Delta \\\\theta$ tends to zero, the approximations become more and more precise,', mode='normal'\n",
      "line='and now one can easily believe that', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{partialftheta}', mode='math'\n",
      "line='\\\\frac{\\\\partial f_{\\\\theta}}{\\\\partial \\\\theta} = ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} + ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta}}{\\\\partial \\\\theta} +', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial', mode='math'\n",
      "line='\\\\theta}.', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='We used a lot of informal derivations with “approximate equal” signs that does', mode='normal'\n",
      "line='not count as a rigorous proof. (Do not try to sell it to your Calculus professor,', mode='normal'\n",
      "line=\"unless it's me!) They can be easily replaced with several\", mode='normal'\n",
      "line='applications of the chain rule, but I want to make clear where each term in this', mode='normal'\n",
      "line='formula came from, and it was easier to do that with the informal picture. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Let's look at the last formula again. We see that to find a derivative of the network with respect to the parameter $\\\\theta$, we have to account for two effects: \", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='1. Change of the parameter $\\\\theta$ affects output of a particular layer. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='2. Change of the output of a layer affects outputs of the subsequent layers, even if we ignore', mode='normal'\n",
      "line='change of the parameter for them. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The first effect is addressed by $\\\\partial f_\\\\theta^i / \\\\partial \\\\theta$', mode='normal'\n",
      "line='multipliers. The second effect is addressed by $\\\\partial f_\\\\theta^i / \\\\partial', mode='normal'\n",
      "line='h^{i-1}$ multipliers. The derivative is a sum of the corresponding effects for', mode='normal'\n",
      "line='each layer.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Back to the gradient', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Now let's use the equation for the derivative of $f$ to find a gradient of $\\\\mathcal\", mode='normal'\n",
      "line='L$. We put $\\\\eqref{partialftheta}$ to $\\\\eqref{nablamathcalL}$ and obtain:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$', mode='math'\n",
      "line='\\\\begin{align*}', mode='math'\n",
      "line='\\\\nabla_{\\\\!\\\\theta} \\\\mathcal L= {} &\\\\nabla_{\\\\!y} L \\\\cdot \\\\frac{\\\\partial', mode='math'\n",
      "line='f_\\\\theta}{\\\\partial \\\\theta}=\\\\\\\\', mode='math'\n",
      "line='& \\\\nabla_{\\\\!y} L \\\\cdot \\\\left(', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} + ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta}}{\\\\partial \\\\theta} +', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial', mode='math'\n",
      "line='\\\\theta}\\\\right)=\\\\\\\\', mode='math'\n",
      "line='& ', mode='math'\n",
      "line='\\\\nabla_{\\\\!y} L \\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} + ', mode='math'\n",
      "line='{\\\\nabla_{\\\\!y} L ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2}}', mode='math'\n",
      "line='\\\\frac{\\\\partial f^2_{\\\\theta}}{\\\\partial \\\\theta} +', mode='math'\n",
      "line='{\\\\nabla_{\\\\!y} L ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2}} ', mode='math'\n",
      "line='\\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial', mode='math'\n",
      "line='\\\\theta}.', mode='math'\n",
      "line='\\\\end{align*}', mode='math'\n",
      "line='$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Note the familiar pattern? In each term, we have vector-row {{< math', mode='normal'\n",
      "line='>}}$\\\\nabla_{\\\\!y}L${{< /math >}} that is multiplied by a sequence of matrices.', mode='normal'\n",
      "line='That means we need to multiply it left-to-right. Moreover, if we look closer, we', mode='normal'\n",
      "line='see there are common parts in the second and the third summands:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$', mode='math'\n",
      "line='\\\\begin{align}', mode='math'\n",
      "line='\\\\nonumber', mode='math'\n",
      "line='\\\\nabla_{\\\\!\\\\theta} \\\\mathcal L(\\\\theta)= ', mode='math'\n",
      "line='\\\\nabla_{\\\\!y} L \\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} & + ', mode='math'\n",
      "line='{\\\\color{teal}\\\\left(\\\\nabla_{\\\\!y} L ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2}\\\\right)}', mode='math'\n",
      "line='\\\\frac{\\\\partial f^2_{\\\\theta}}{\\\\partial \\\\theta} \\\\\\\\', mode='math'\n",
      "line='\\\\label{nablamcL}', mode='math'\n",
      "line='&+ {\\\\color{teal} \\\\left(\\\\nabla_{\\\\!y} L ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2}\\\\right)} ', mode='math'\n",
      "line='\\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta}}{\\\\partial h_1}  \\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial', mode='math'\n",
      "line='\\\\theta}.', mode='math'\n",
      "line='\\\\end{align}', mode='math'\n",
      "line='$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='It means that we can find this common part {{< math >}}$\\\\nabla_{\\\\!y} L ', mode='normal'\n",
      "line='\\\\cdot \\\\partial f^3_{\\\\theta}/\\\\partial h_2${{< /math >}} when calculate the second', mode='normal'\n",
      "line=\"summand, and then reuse it when calculating the third summand. That's allows us\", mode='normal'\n",
      "line='to do the calculations even more efficiently. And this is not a coincidence: the', mode='normal'\n",
      "line='same trick works in deeper networks as well!', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### General algorithm for backpropagation', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Previously we considered a network with three layers. Now I want to', mode='normal'\n",
      "line='generalize the formula for loss gradient to the general case of the network with $N$ layers.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Note that in each summand on the right-hand side of equation $\\\\eqref{nablamcL}$ only', mode='normal'\n",
      "line='the last multiplier is a derivative with respect to the parameters $\\\\theta$. The', mode='normal'\n",
      "line='beginning part of each product is a gradient of “truncated composition” like in', mode='normal'\n",
      "line='$\\\\eqref{truncated}$ with respect to the output of some of the hidden layer.', mode='normal'\n",
      "line='Indeed, the chain rule implies:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\nabla_{\\\\!y} L ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} = ', mode='math'\n",
      "line='\\\\nabla_{\\\\!h_2}(L\\\\circ f^3_\\\\theta)', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='and', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\nabla_{\\\\!y} L ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial h_2} \\\\frac{\\\\partial f^2_\\\\theta}{\\\\partial', mode='math'\n",
      "line='h_1} = ', mode='math'\n",
      "line='\\\\nabla_{\\\\!h_1}(L\\\\circ f^3_\\\\theta \\\\circ f^2_\\\\theta).', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='In other words, these multipliers show how the loss function depends on the', mode='normal'\n",
      "line='output value of the second and the first hidden layers correspondingly. To', mode='normal'\n",
      "line='simplify the notation, we will write {{< math >}}$\\\\nabla_{\\\\!h_2} L${{< /math >}} and {{< math >}}$\\\\nabla_{\\\\!h_1} L${{< /math >}}', mode='normal'\n",
      "line='and omit the subsequent compositions with the layer maps. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Let us denote $h_3 \\\\equiv y$. Then the following nice relations take place:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{align}', mode='math'\n",
      "line='\\\\label{nablaLstep1}', mode='math'\n",
      "line='\\\\nabla_{\\\\!h_2} L & = \\\\nabla_{\\\\!h_3} L \\\\cdot  ', mode='math'\n",
      "line='    \\\\frac{\\\\partial f^3_\\\\theta}{\\\\partial h_2}, \\\\\\\\', mode='math'\n",
      "line='\\\\label{nablaLstep2}', mode='math'\n",
      "line='\\\\nabla_{\\\\!h_1} L & = \\\\nabla_{\\\\!h_2} L \\\\cdot ', mode='math'\n",
      "line='    \\\\frac{\\\\partial f^2_\\\\theta}{\\\\partial h_1}.', mode='math'\n",
      "line='\\\\end{align}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='This is actually just a restatement of the general equation', mode='normal'\n",
      "line='$\\\\eqref{nablastep}$ for truncated compositions.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='With this new notation, we can rewrite the formula for the gradient', mode='normal'\n",
      "line='$\\\\eqref{nablamcL}$ in the following compact way:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\nabla_{\\\\!\\\\theta} \\\\mathcal L(\\\\theta)=', mode='math'\n",
      "line='\\\\nabla_{\\\\!h_3} L \\\\frac{\\\\partial f^3_{\\\\theta}}{\\\\partial \\\\theta} + ', mode='math'\n",
      "line='\\\\nabla_{\\\\!h_2} L ', mode='math'\n",
      "line='\\\\frac{\\\\partial', mode='math'\n",
      "line='f^2_{\\\\theta}}{\\\\partial \\\\theta} +', mode='math'\n",
      "line='\\\\nabla_{\\\\!h_1} L ', mode='math'\n",
      "line='\\\\frac{\\\\partial f^1_{\\\\theta}}{\\\\partial', mode='math'\n",
      "line='\\\\theta}.', mode='math'\n",
      "line='$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='And this can be easily generalized to the case of $N$ layers:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{nabla-L-sum}', mode='math'\n",
      "line='\\\\nabla_{\\\\!\\\\theta} \\\\mathcal L(\\\\theta)=\\\\sum_{i=N}^1 \\\\nabla_{\\\\!h_i}L \\\\frac{\\\\partial', mode='math'\n",
      "line='f_\\\\theta^i}{\\\\partial \\\\theta},', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where $h_N\\\\equiv y$. (I am intentionally start the summation from $i=N$ and then', mode='normal'\n",
      "line='decrease $i$ until it equals $1$ for consistency with the previous equation and', mode='normal'\n",
      "line='the algorithm below.) This equation looks simple, and, moreover, there exists', mode='normal'\n",
      "line='efficient algorithm to calculate its right-hand side. First, note that equations', mode='normal'\n",
      "line='$\\\\eqref{nablaLstep1}$-$\\\\eqref{nablaLstep2}$ are immediately generalized as', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{nablaLstep}', mode='math'\n",
      "line='\\\\nabla_{\\\\!h_i} L  = \\\\nabla_{\\\\!h_{i+1}} L \\\\cdot  ', mode='math'\n",
      "line='    \\\\frac{\\\\partial f^{i+1}_\\\\theta}{\\\\partial h_i}, \\\\quad i = N, \\\\ldots, 1.', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='(Again, this is just equation $\\\\eqref{nablastep}$ with different notation.)', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Then we have the following algorithm:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='1. Do the forward pass to find values $h_1$, $h_2$, …, $h_N\\\\equiv y$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='2. Initialize the accumulator to store the gradient with zero $p$-dimensional', mode='normal'\n",
      "line='   vector-row. (Recall that $p$ is the number of parameters.)', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='3. Find {{< math >}}$\\\\nabla_{\\\\\\\\!y} L(y)${{< /math >}}.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='4. Find {{< math >}}$(\\\\nabla_{\\\\\\\\!y} L) (\\\\partial f^N_\\\\theta(h_{N-1}) / \\\\partial \\\\theta)${{< /math >}} and add it to the accumulator.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='5. For each $i$ from $(N - 1)$ to $1$:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='    - Find {{< math >}}$\\\\nabla_{\\\\\\\\!h_i} L${{< /math >}} by multiplication of', mode='normal'\n",
      "line='        the previously found {{< math >}}$\\\\nabla_{\\\\\\\\!h_{i+1}} L${{< /math >}} to the', mode='normal'\n",
      "line='        derivative {{< math >}}$\\\\partial f^{i+1}_\\\\theta(h\\\\_{i}) / \\\\partial h\\\\_{i}${{< /math >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='    - Find {{< math >}}$(\\\\nabla_{\\\\\\\\! h_i} L) (\\\\partial f^{i}\\\\_\\\\theta(h_{i-1}) / \\\\partial \\\\theta)${{< /math >}} and add it to the accumulator.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='6. Return the value of the accumulator.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"That's it. That's how backpropagation allows to efficiently calculate gradients\", mode='normal'\n",
      "line=\"in the usual neural networks. Now let's pass to neural ODEs.\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='## Adjoint State Method in Neural ODEs', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Neural ODEs: quick recap', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='As we discussed previously, during the forward pass, the usual neural network', mode='normal'\n",
      "line='transforms its inputs to outputs in a sequence of discrete steps: one step', mode='normal'\n",
      "line='corresponds to one layer. In neural ODEs, this transformation is performed', mode='normal'\n",
      "line=\"continously. Now we don't have a discrete set of layers, enumerated by natural\", mode='normal'\n",
      "line='numbers $i=1, \\\\ldots, N$. Instead, we have a continuum set of “moments of time”,', mode='normal'\n",
      "line='represented as a segment $[0, T]$.  At each moment, we specify “infinitesimal', mode='normal'\n",
      "line='transformation” that occurs when the value passes through this moment.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Technically, neural ODEs are obtained as a limit case of so-called _residual', mode='normal'\n",
      "line=\"networks_ (also known as _ResNets_). In the residual networks, the output value of $i$'th layer is determined as\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='h_{i}=h_{i-1} + f^{i}_\\\\theta(h_{i-1}).', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='The difference with the usual neural networks is the presence of “$h_{i-1}+{}$” term.', mode='normal'\n",
      "line='It allows the network to learn more efficiently: ResNets can be very deep and', mode='normal'\n",
      "line='still learnable. (Note that to write such an equation we must demand that the', mode='normal'\n",
      "line='dimensionality of each layer be the same and equal to the dimensionality of the', mode='normal'\n",
      "line='input space.) Now we can imagine a very-very deep network: to make sure that', mode='normal'\n",
      "line=\"the output doesn't tend to infinity, let's add some small coefficient that will\", mode='normal'\n",
      "line='decrease as the network depth increases:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='h_{i}=h_{i-1} + \\\\varepsilon f^{i}_\\\\theta(h_{i-1}), \\\\quad i=1,\\\\ldots, N,', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line=\"where $\\\\varepsilon \\\\sim 1/N$. And that's an equation of well-known [Euler\", mode='normal'\n",
      "line='method](https://en.wikipedia.org/wiki/Euler_method) of the numerical solution of', mode='normal'\n",
      "line='a differential equation! As $N$ tends to infinity, the sequence of values $h_i$', mode='normal'\n",
      "line=\"tends to a solution of the corresponding differential equation. That's the\", mode='normal'\n",
      "line='rationale.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Now the formal settings. Consider a differential equation', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\dot x(t) = f_\\\\theta(t, x(t)),', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where $x$ is a function, $x(t) \\\\in \\\\mathbb R^n$, $n$ is the dimensionality of the input space, $\\\\dot x(t)$ is a derivative $dx(t)/dt$, $f_\\\\theta$ is a smooth', mode='normal'\n",
      "line='function that depends on the vector of parameters $\\\\theta$. Denote the input of', mode='normal'\n",
      "line='the network by $x_{input} \\\\in \\\\mathbb R^n.$ Consider a point $(t=0,', mode='normal'\n",
      "line=\"x=x_{input})$: that's our starting point. We can find a solution of the\", mode='normal'\n",
      "line=\"differential equation whose graph passes through this point. Let's denote this\", mode='normal'\n",
      "line='solution by $\\\\varphi(t; x_{input}; \\\\theta)$. By construction, $\\\\varphi(0;', mode='normal'\n",
      "line='x_{input}; \\\\theta)=x_{input}$.  The output of the network, by definition, is the value', mode='normal'\n",
      "line='of this solution at moment $T$, where $T$ is some fixed positive number. If one', mode='normal'\n",
      "line='changes the initial value $x_{input}$, the solution changes as well and so does', mode='normal'\n",
      "line='the output. In other words, our network defines a map', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{align*}', mode='math'\n",
      "line='g^{0:T}_\\\\theta & \\\\colon \\\\mathbb R^n  \\\\to \\\\mathbb R^n, \\\\\\\\', mode='math'\n",
      "line='g^{0:T}_\\\\theta & (x_{input}) =\\\\varphi(T; x_{input};\\\\theta),', mode='math'\n",
      "line='\\\\end{align*}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='also known as _phase flow map_ of our differential equation.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-8.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 9. The usual neural network vs. neural ODE\"', mode='normal'\n",
      "line='>}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='There is a “dictionary” between the usual neural networks and neural ODEs. The moment of time $t$ corresponds to the index of layer $i$. For a given $t$, the function', mode='normal'\n",
      "line='$f_\\\\theta(t, x)$, seen as a function of $x$, corresponds to the layer function', mode='normal'\n",
      "line='$f^{i}\\\\_\\\\theta$. The phase flow map $g^{0:T}\\\\_\\\\theta$ corresponds to the composition of the layer functions $f^N_\\\\theta \\\\circ \\\\cdots \\\\circ f^1\\\\_\\\\theta$ (compare also with the notation in the section [Truncated compositions](#truncated-compositions)).', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Okay, now if we have a differential equation, we have a corresponding map from', mode='normal'\n",
      "line='inputs to outputs. But how we define this differential equation? How to define', mode='normal'\n",
      "line=\"the function $f_\\\\theta(t, x)$. And that's easy: let's say it is given by another\", mode='normal'\n",
      "line='(usual) neural network! In this case, $\\\\theta$ is a vector of parameters of this network', mode='normal'\n",
      "line='(weights and biases), and it determines the function $f_\\\\theta$ and therefore', mode='normal'\n",
      "line='determines the map $g^{0:T}_\\\\theta$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='How to learn $\\\\theta$?', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Derivative of the solution of ODE', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='As usual, we have some loss function $L(y, y_{true})$ and can define our', mode='normal'\n",
      "line='training objective as', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\mathcal L(\\\\theta)=L(g^{0:T}_\\\\theta(x_{input}), y_{true})', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='and its gradient is immediately given by', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{nablaLode}', mode='math'\n",
      "line='\\\\nabla_{\\\\! \\\\theta} \\\\mathcal L(\\\\theta) = \\\\nabla_{\\\\! y}L\\\\cdot \\\\frac{\\\\partial g^{0:T}_\\\\theta(x_{input})}{\\\\partial \\\\theta}.', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='The second multiplier in the right-hand part can be also written as', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\frac{\\\\partial g^{0:T}_\\\\theta(x_{input})}{\\\\partial \\\\theta}=\\\\frac{\\\\partial', mode='math'\n",
      "line='\\\\varphi(x_{input}; T; \\\\theta)}{\\\\partial \\\\theta},', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='so we are interested in the derivative of the solution of a differential equation', mode='normal'\n",
      "line='with respect to the parameter.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='It is a well-studied topic in the theory of ODE. We have a theorem here.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='**Theorem.** For each $t\\\\in [0, T]$ and some fixed $\\\\theta$, let us denote', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='v(t) := \\\\frac{\\\\partial \\\\varphi(x\\\\_{input}; t; \\\\theta)}{\\\\partial \\\\theta}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='So $v(t)$ measures how the solution of our equation at point $t$ depends on the', mode='normal'\n",
      "line='parameters $\\\\theta.$ Then (under reasonable assumptions) $v$ satisfies the following differential equation:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{var-u-theta}', mode='math'\n",
      "line='\\\\dot v = \\\\frac{\\\\partial f\\\\_\\\\theta(t, x(t))}{\\\\partial \\\\theta} + \\\\frac{\\\\partial', mode='math'\n",
      "line='f_\\\\theta(t, x(t))}{\\\\partial x} v,', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where $x(t)=\\\\varphi(t; x\\\\_{input}; \\\\theta)$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< spoiler ', mode='normal'\n",
      "line='text=\"What initial condition on $v(0)$ should we impose? How do you think?\" >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='We know by definition, that $\\\\varphi(x_{input}; 0; \\\\theta)=x_{input}$ and', mode='normal'\n",
      "line='therefore does not depend on $\\\\theta$. So the initial condition is $v(0)=0$', mode='normal'\n",
      "line='(zero matrix of appropriate size).', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /spoiler >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Now we can solve equation $\\\\eqref{var-u-theta}$, find $v(T)$ and that's all. What's the\", mode='normal'\n",
      "line='problem with this approach?', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='In fact, $v(t)$ is a large $(n\\\\times p)$-matrix, it contains the derivatives of', mode='normal'\n",
      "line='all $n$ components of the solution with respect to every parameter. It can be expensive to', mode='normal'\n",
      "line=\"calculate this matrix, especially if $n$ is large. And we don't need it by itself:\", mode='normal'\n",
      "line='what we are interested in is the product of this matrix to $\\\\nabla_{\\\\\\\\! y}L$.', mode='normal'\n",
      "line='Basically, to solve this equation is like to find a product in equation', mode='normal'\n",
      "line='$\\\\eqref{nablaG}$ from right to left: possible, but not optimal. How to do it in', mode='normal'\n",
      "line='a smart way? Of course, with backpropagation!', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Perturbed trajectories', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='We will follow the ideas discussed in the section [Derivative of the', mode='normal'\n",
      "line='network](#derivative-of-the-network). First, let us consider a network with', mode='normal'\n",
      "line='perturbed vector of parameters, i.e. instead of $f(t, x, \\\\theta)$ we consider an', mode='normal'\n",
      "line='equation given by $f(t, x, \\\\theta+\\\\Delta \\\\theta)$ for some small value of', mode='normal'\n",
      "line='$\\\\Delta \\\\theta$. Consider two solutions that pass through the same point $(0,', mode='normal'\n",
      "line='x_{input})$: the initial solution $x(t; \\\\theta)$ and the perturbed', mode='normal'\n",
      "line='solution $x(t; \\\\theta+\\\\Delta \\\\theta)$. (From now on, we fixed the input', mode='normal'\n",
      "line='value and skip $x_{input}$ in the notation.) We also introduce', mode='normal'\n",
      "line='pertubed phase flow $g^{0:T}_{\\\\theta+\\\\Delta \\\\theta}$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The difference between the solutions at moment $T$ can be approximated using the ', mode='normal'\n",
      "line='derivative of the phase flow map with respect to $\\\\theta$:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\varphi(T; \\\\theta+\\\\Delta \\\\theta)-\\\\varphi(T; \\\\theta)  = ', mode='math'\n",
      "line='g^{0:T}_{\\\\theta+\\\\Delta \\\\theta}(x_{input}) -', mode='math'\n",
      "line='g^{0:T}_{\\\\theta}(x_{input}) \\\\approx ', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{0:T}_\\\\theta}{\\\\partial \\\\theta} \\\\Delta \\\\theta.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-9.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 10. The initial and the perturbed solutions\"', mode='normal'\n",
      "line=\"id='initial-pert'\", mode='normal'\n",
      "line='>}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Our idea is to decompose this difference into sum of smaller differences like we', mode='normal'\n",
      "line='did in the section [Derivative of the', mode='normal'\n",
      "line='network](#derivative-of-the-network) (see [Figure 8](#figure-decomp)). For this, we need to', mode='normal'\n",
      "line='introduce a bit more notation.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Consider arbitrary moment $t^* \\\\in [0, T]$ and a point {{< math >}}$(t^*, x(t^*;', mode='normal'\n",
      "line='\\\\theta))${{< /math >}} that lies on the graph of the unperturbed solution (also', mode='normal'\n",
      "line='called _unperturbed trajectory_).', mode='normal'\n",
      "line='Sometimes for brevity we will skip $\\\\theta$ in the notation for the unperturbed', mode='normal'\n",
      "line=\"solutions and write simply $x(t^\\\\*)$. Now let's consider a solution of\", mode='normal'\n",
      "line='_perturbed_ system through this point, see [Figure 11](#figure-additional-perturbed) below. The value of this solution', mode='normal'\n",
      "line='at point $T$ is denoted by', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='g^{t^*:T}_{\\\\theta+\\\\Delta \\\\theta}(x(t^*)),', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='thus giving us a flow map that transforms points over $t^*$ to points over $T$', mode='normal'\n",
      "line='under the action of the perturbed system.', mode='normal'\n",
      "line='(Compare the notation with equation $\\\\eqref{truncated}$ and Figure 3 at the section [Truncated', mode='normal'\n",
      "line='compositions](#truncated-compositions).)', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-10.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 11. Additional perturbed trajectory\"', mode='normal'\n",
      "line=\"id='additional-perturbed'\", mode='normal'\n",
      "line='>}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Now we are ready to construct decomposition like in [Figure', mode='normal'\n",
      "line=\"6](#figure-decomp-net). Let's do it!\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Derivative decomposition', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Let us divide the segment $[0, T]$ into $K$ smaller segments of equal length and', mode='normal'\n",
      "line='denote the endpoints of these segments by $t_0$, $t_1$, …, $t_K$, where $t_0=0$', mode='normal'\n",
      "line='and $t_K=T$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-11.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 12. Many perturbed trajectories\"', mode='normal'\n",
      "line='id=\"many-perturbed\"', mode='normal'\n",
      "line='>}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='For each point $(t_j, x(t_j))$, we consider a solution of the perturbed system', mode='normal'\n",
      "line='through this point. The corresponding trajectories divide the segment', mode='normal'\n",
      "line='{{< math >}}$[g^{0:T}_\\\\theta(x_{input}), g^{0:T}_{\\\\theta+\\\\Delta \\\\theta}(x_{input})]${{< /math >}} into $K$ smaller segments. Let us denote them by $\\\\Delta\\\\_1, \\\\ldots, \\\\Delta\\\\_K$. They are direct counterparts of the segments $\\\\Delta^3_1$, $\\\\Delta^3_2$, and $\\\\Delta^3_3$ defined [above](#derivative-of-the-network), see [Figure 8](#figure-decomp-net). For each integer $j=1, 2, \\\\ldots, K-1$, we are also interested in the segment of the line $t=t_j$ between point $x(t_j)$ lying on the unperturbed trajectory and the perturbed trajectory through the point $(t_{j-1}, x(t_{j-1}))$. Denote them by $\\\\tilde \\\\Delta_j$. They are direct counterparts of segments $\\\\Delta_1^1$ and $\\\\Delta_2^2$ on [Figure 8](#figure-decomp-net). Note that each $\\\\Delta_j$ is an image of $\\\\tilde \\\\Delta_j$ under the map $g^{t_j:T}_{\\\\theta+\\\\Delta \\\\theta}$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Again, there's a catch: $\\\\Delta_j$'s are not segments, they are\", mode='normal'\n",
      "line='vectors in the multidimensional space, and strictly speaking they cannot “split”', mode='normal'\n",
      "line='the segment $[g^{0:T}\\\\_{\\\\theta }(x\\\\_{input}), ', mode='normal'\n",
      "line='g^{0:T}\\\\_{\\\\theta+\\\\Delta \\\\theta}(x\\\\_{input})]$.', mode='normal'\n",
      "line='Nevertheless, their sum is equal to the vector $g^{0:T}\\\\_{\\\\theta+\\\\Delta \\\\theta}(x\\\\_{input}) -', mode='normal'\n",
      "line='g^{0:T}_{\\\\theta}(x\\\\_{input})$, and this is actual assertion we will use.', mode='normal'\n",
      "line=\"However, it is easier to think about “segments”, so I'll keep this terminology.\", mode='normal'\n",
      "line='The same holds for $\\\\tilde \\\\Delta_j$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Now we can write:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{dgdtheta}', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{0:T}_\\\\theta(x_{input})}{\\\\partial \\\\theta} \\\\Delta \\\\theta \\\\approx', mode='math'\n",
      "line='\\\\sum_{j=1}^{K} \\\\Delta_j.', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line=\"The rest is to investigate $\\\\Delta_j$'s.\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Smaller segments', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='As it was noted, each $\\\\Delta_j$ is an image of $\\\\tilde \\\\Delta_j$', mode='normal'\n",
      "line='under the phase flow map $g^{t_j:T}_{\\\\theta+\\\\Delta \\\\theta}$. If $K$ is large, these', mode='normal'\n",
      "line='segments are small, and the phase flow map can be approximated by its', mode='normal'\n",
      "line='derivative. Therefore,', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{Delta-star}', mode='math'\n",
      "line='\\\\Delta_j \\\\approx \\\\frac{\\\\partial g^{t_j: T}_{\\\\theta+\\\\Delta \\\\theta}(x(t_j))}{\\\\partial x} \\\\tilde \\\\Delta_j.', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='To find $\\\\tilde \\\\Delta_{j}$, let us approximate the unperturbed and perturbed trajectories', mode='normal'\n",
      "line='throught the point $(t_{j-1}, x(t_{j-1}))$ by their respective', mode='normal'\n",
      "line='tangent lines at this point, see [Figure 13](#figure-approx-traj).', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< figure src=\"/img/adjoint-state/backprop-13.svg\" width=\"90%\" ', mode='normal'\n",
      "line='title=\"Figure 13. Approximation trajectories with tangents\"', mode='normal'\n",
      "line='id=\"approx-traj\"', mode='normal'\n",
      "line='>}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The trajectories are graphs of the solutions of the equations with right-hand', mode='normal'\n",
      "line='side given by $f_{\\\\theta+\\\\Delta \\\\theta}$ and $f_\\\\theta$. The slopes of the tangent lines thus are equal to the value of these functions:', mode='normal'\n",
      "line='$f_{\\\\theta+\\\\Delta \\\\theta}(t_{j-1}, x(t_{j-1}))$ and $f_{\\\\theta}(t_{j-1},', mode='normal'\n",
      "line='x(t_{j-1}))$. Therefore, the length of the segment over $t_j$ cut by', mode='normal'\n",
      "line='tangents is equal to', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\bar \\\\Delta_j = (f_{\\\\theta+\\\\Delta \\\\theta}(t_{j-1}, x(t_{j-1})) -', mode='math'\n",
      "line='f_{\\\\theta}(t_{j-1},', mode='math'\n",
      "line='x(t_{j-1}))) \\\\Delta t_j,', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where $\\\\Delta t_j=t_j-t_{j-1}$.', mode='normal'\n",
      "line='For large $K$, $\\\\Delta t_j$ is small and the actual trajectories lie close to the respective tangent lines, and thus   $\\\\tilde \\\\Delta_j \\\\approx \\\\bar \\\\Delta_j$. Now for small $\\\\Delta \\\\theta$, we can approximate $(f_{\\\\theta+\\\\Delta \\\\theta}(t_{j-1}, x(t_{j-1})) - f_{\\\\theta}(t_{j-1}, x(t_{j-1})))$ by the corresponding derivative, and obtain:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\tilde \\\\Delta_j \\\\approx \\\\frac{\\\\partial f_\\\\theta(t_{j-1}, x(t_{j-1}))}{\\\\partial', mode='math'\n",
      "line='\\\\theta} \\\\Delta \\\\theta \\\\Delta t_j.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Put it into equation $\\\\eqref{Delta-star}$ and obtain:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\Delta_j \\\\approx ', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{t_j: T}_{\\\\theta+\\\\Delta \\\\theta}(x(t_j))}{\\\\partial x} ', mode='math'\n",
      "line='\\\\frac{\\\\partial f_\\\\theta(t_{j-1}, x(t_{j-1}))}{\\\\partial \\\\theta} \\\\Delta \\\\theta\\\\, \\\\Delta t_j.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Now return to equation $\\\\eqref{dgdtheta}$. We have:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\frac{\\\\partial g^{0:T}_\\\\theta(x_{input})}{\\\\partial \\\\theta} \\\\Delta \\\\theta \\\\approx', mode='math'\n",
      "line='\\\\sum_{j=1}^K ', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{t_j: T}_{\\\\theta+\\\\Delta \\\\theta}(x(t_j))}{\\\\partial x} ', mode='math'\n",
      "line='\\\\frac{\\\\partial f_\\\\theta(t_{j-1}, x(t_{j-1}))}{\\\\partial \\\\theta} \\\\Delta \\\\theta\\\\, \\\\Delta t_j', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Clearly, it looks pretty much like an integral sum! Thus it is easy to believe in the following', mode='normal'\n",
      "line='approximation:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{align*}', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{0:T}_\\\\theta(x_{input})}{\\\\partial \\\\theta} \\\\Delta \\\\theta \\\\approx &', mode='math'\n",
      "line='\\\\int_{0}^T ', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{t: T}_{\\\\theta+\\\\Delta \\\\theta}(x(t))}{\\\\partial x} ', mode='math'\n",
      "line='\\\\frac{\\\\partial f_\\\\theta(t, x(t))}{\\\\partial \\\\theta} \\\\Delta \\\\theta \\\\, dt= \\\\\\\\', mode='math'\n",
      "line='& \\\\left(\\\\int_0^T \\\\frac{\\\\partial g^{t: T}_{\\\\theta+\\\\Delta \\\\theta}(x(t))}{\\\\partial x} ', mode='math'\n",
      "line='\\\\frac{\\\\partial f_\\\\theta(t, x(t))}{\\\\partial \\\\theta} \\\\, dt\\\\right) \\\\Delta \\\\theta.', mode='math'\n",
      "line='\\\\end{align*}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='As $\\\\Delta \\\\theta$ becomes small, this equality becomes more and more exact. It', mode='normal'\n",
      "line='holds for any small $\\\\Delta \\\\theta$, therefore, the following (now exact) equality on the', mode='normal'\n",
      "line='derivative takes place:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{dgdtheta-int}', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{0:T}_\\\\theta(x_{input})}{\\\\partial \\\\theta} =', mode='math'\n",
      "line='\\\\int_0^T \\\\frac{\\\\partial g^{t: T}_{\\\\theta}(x(t))}{\\\\partial x} ', mode='math'\n",
      "line='\\\\frac{\\\\partial f_\\\\theta(t, x(t))}{\\\\partial \\\\theta} \\\\, dt.', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='This equation is a continuous counterpart of equation \\\\eqref{partialftheta}.', mode='normal'\n",
      "line='Indeed, the first multiplier corresponds to a derivative of the output of the', mode='normal'\n",
      "line='network with', mode='normal'\n",
      "line='respect to the output of some intermediate layer. The second multiplier gives the', mode='normal'\n",
      "line='dependency of the output of intermediate layer with respect to the parameter.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The derivation above is even less rigorous and more risky than the previous one.', mode='normal'\n",
      "line='We deal with two limits here, $\\\\Delta t \\\\to 0$ and $\\\\Delta \\\\theta \\\\to 0$, and', mode='normal'\n",
      "line='this is a red flag for everyone who studied Calculus: intuition can easily fool', mode='normal'\n",
      "line='us here. I present this handwaving only because I know the actual proof and', mode='normal'\n",
      "line='absolutely sure everything is OK. At the same time, I believe that this kind of', mode='normal'\n",
      "line='approximate derivations and plots like [Figure 12](#figure-many-perturbed) ', mode='normal'\n",
      "line='allows us to _understand_ what', mode='normal'\n",
      "line='is really going on, and the formal proof is just a check that our intuition', mode='normal'\n",
      "line='still works correctly.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Okay, you may ask, we obtained a new formula for the derivative of the output of', mode='normal'\n",
      "line='the network with respect to the parameters. But we discussed previously that it', mode='normal'\n",
      "line=\"can be expensive to find it, and we don't actually need it. How this new formula\", mode='normal'\n",
      "line='helps us?', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Glad you asked! We are ready for an answer.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### Back to the gradient revisited', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Let us put \\\\eqref{dgdtheta-int} into \\\\eqref{nablaLode}:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}', mode='normal'\n",
      "line='$$', mode='math'\n",
      "line='\\\\begin{align*}', mode='math'\n",
      "line='\\\\nabla_{\\\\! \\\\theta} \\\\mathcal L(\\\\theta) & =   \\\\nabla_{\\\\! y}L \\\\cdot \\\\int_0^T \\\\frac{\\\\partial g^{t: T}_{\\\\theta}(x(t))}{\\\\partial x} ', mode='math'\n",
      "line='\\\\frac{\\\\partial f_\\\\theta(t, x(t))}{\\\\partial \\\\theta} \\\\, dt  \\\\\\\\', mode='math'\n",
      "line='& = \\\\int_0^T \\\\left(  \\\\nabla_{\\\\! y}L \\\\frac{\\\\partial g^{t: T}_{\\\\theta}(x(t))}{\\\\partial x} \\\\right)', mode='math'\n",
      "line='\\\\frac{\\\\partial f_\\\\theta(t, x(t))}{\\\\partial \\\\theta} \\\\, dt.', mode='math'\n",
      "line='\\\\end{align*}', mode='math'\n",
      "line='$$', mode='math'\n",
      "line='{{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Let us consider the first multiplier. Note that $\\\\nabla_{\\\\\\\\! y}L$ is the gradient', mode='normal'\n",
      "line='of function $L$ calculated at point $y=x(T)$. It measures how $L$ depends on the', mode='normal'\n",
      "line='output of the network. At the same time, $\\\\partial g_{\\\\theta}^{t:T}/\\\\partial x$ measures', mode='normal'\n",
      "line='how output of the network depends on $x(t)$, i.e. ', mode='normal'\n",
      "line='“output of layer $t$”. Thus, by the chain rule, the product of these', mode='normal'\n",
      "line='derivatives measures how $L$ depends on $x(t)$.', mode='normal'\n",
      "line='One may write:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\nabla_{\\\\! y}L \\\\frac{\\\\partial g^{t: T}_{\\\\theta}(x(t))}{\\\\partial x} =', mode='math'\n",
      "line='\\\\nabla_{\\\\! x}(L\\\\circ g^{t:T}_\\\\theta(x)),', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where the gradient is taken at point $x=x(t)$. Let us introduce a bit informal', mode='normal'\n",
      "line='notation:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\nabla_{\\\\! x}(L\\\\circ g^{t:T}_\\\\theta(x))=:\\\\nabla_{\\\\! x(t)} L.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='It is a counterpart of $\\\\nabla_{\\\\\\\\! h_i} L$ in the section [General algorithm for backpropagation](general-algorithm-for-backpropagation). With this notation, the integral above can be written in the following form:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{nabla-L-int}', mode='math'\n",
      "line='\\\\nabla_{\\\\! \\\\theta} \\\\mathcal L(\\\\theta) = \\\\int_0^T  \\\\nabla_{\\\\! x(t)}L \\\\frac{\\\\partial f_\\\\theta(t, x(t))}{\\\\partial \\\\theta} \\\\, dt.', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line=\"And it looks very similar to equation \\\\eqref{nabla-L-sum}, isn't it? Note that we don't\", mode='normal'\n",
      "line='have the large matrix derivative $\\\\partial g_\\\\theta^{t:T}(x) / \\\\partial x$ in the formula anymore:', mode='normal'\n",
      "line='it was “swallowed“ by the gradient $\\\\nabla_{\\\\\\\\! x(t)} L$. Looks like a good news!', mode='normal'\n",
      "line='But are there any good ways to find this gradient?', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='In the usual backpropagation, we used recurrent equation \\\\eqref{nablaLstep} to', mode='normal'\n",
      "line='find $\\\\nabla_{\\\\\\\\! h_i} L$ one by one (the backward pass). In the continous setting, we', mode='normal'\n",
      "line=\"don't have such a recurrence. Looks like a bad news. But don't worry, a bit of\", mode='normal'\n",
      "line='ODE magic will help us!', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='### The adjoint equation', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='We are so close! Just a couple of steps ahead. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Let's denote $\\\\nabla_{\\\\\\\\! x(t)} L$ by $a(t)$. This is a vector-row\", mode='normal'\n",
      "line='(covector). It is called _adjoint state_ or simply', mode='normal'\n",
      "line='_adjoint_. It depends on $t$, and I feel it should satisfy some differential', mode='normal'\n",
      "line='equation. How to find this equation?', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='From now on, we omit the dependence on $\\\\theta$ in the notation. For any $t \\\\in [0, T]$ and for any input value $x_0$, let us consider the following decomposition:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='g^{0:T}(x_0)=g^{t:T} \\\\circ g^{0:t}(x_0).', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='This formula says that to find', mode='normal'\n",
      "line='the output of the network for the input $x_0$, we have to find the output value of', mode='normal'\n",
      "line=\"the intermediate layer $t$ for input $x_0$ (that's $g^{0:t}(x_0)$), and then pass it to\", mode='normal'\n",
      "line=\"the rest of the network ($g^{t:T}$). So that's a trivial identity that holds for\", mode='normal'\n",
      "line='any $t$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Let's add $L$ to the left:\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='L \\\\circ g^{0:T}(x_0)=L \\\\circ g^{t:T} \\\\circ g^{0:t}(x_0).', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Then take a gradient with respect to $x_0$ and use the chain rule:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\nabla_{\\\\! x_0} (L \\\\circ g^{0:T}(x_0)) = \\\\nabla_{\\\\! x} (L \\\\circ g^{t:T})', mode='math'\n",
      "line='\\\\frac{\\\\partial g^{0: t}(x_0)}{\\\\partial x_0}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Clearly, the left-hand side is $\\\\nabla_{\\\\\\\\! x(0)} L$, i.e. $a(0)$, and the first', mode='normal'\n",
      "line='multiplier of the right-hand side is $\\\\nabla_{\\\\\\\\! x(t)} L=a(t)$. Therefore, one', mode='normal'\n",
      "line='have:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='a(0) = a(t)\\\\frac{\\\\partial g^{0: t}(x_0)}{\\\\partial x_0}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line=\"Now let's take a derivative with respect to $t$. The left-hand side does not\", mode='normal'\n",
      "line='depend on $t$, so the derivative is 0. At the right-hand side, one have a', mode='normal'\n",
      "line='product of two time-dependent matrices, so [Leibniz product rule](https://en.wikipedia.org/wiki/Product_rule) should be applied:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='0 = \\\\dot a(t)\\\\frac{\\\\partial g^{0: t}(x_0)}{\\\\partial x_0}+a(t) \\\\frac{d}{dt} \\\\frac{\\\\partial g^{0: t}(x_0)}{\\\\partial x_0}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Note that $g^{0:t}(x_0)$, seen as a function of $t$, is just a solution of our', mode='normal'\n",
      "line='equation with the initial condition $x(0)=x_0$:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='g^{0:t}(x_0)=\\\\varphi(t; x_0).', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='So the derivative $\\\\partial g^{0:t}(x_0)/\\\\partial x_0$ is well-known as', mode='normal'\n",
      "line='a ”derivative of the solution with respect to the initial conditions”. And we have', mode='normal'\n",
      "line='a theorem here.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< callout note >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='**Theorem.** Consider differential equation', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\dot x = f(t, x), \\\\quad x(t) \\\\in \\\\mathbb R^n,', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='and let $x=\\\\varphi(t; x_0)$ be its solution with initial condition', mode='normal'\n",
      "line='$x(0)=x_0$. Let us denote', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='w(t):=\\\\frac{\\\\partial \\\\varphi(t; x_0)}{\\\\partial x_0},', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where $w$ is $(n\\\\times n)$-matrix. Then $w$ satisfies the following linear equation:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\dot w = \\\\frac{\\\\partial f(t, x)}{\\\\partial x} w,', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='where derivative is taken at point $x=\\\\varphi(t; x_0)$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /callout >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< spoiler text=\"Wanna proof? Click here!\" >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='**Proof.** It is rather simple if we believe that $\\\\varphi$ depends smoothly on', mode='normal'\n",
      "line='$t$ and $x_0.$', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Let's find a derivative of $w(t)$ with respect to $t$. Note that $w$ is itself a\", mode='normal'\n",
      "line='derivative, and we can change the order of differentiation (if we believe in', mode='normal'\n",
      "line='smoothness):', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\dot w(t) = \\\\frac{\\\\partial}{\\\\partial t}\\\\frac{\\\\partial \\\\varphi(t; x_0)}{\\\\partial x_0}=', mode='math'\n",
      "line='\\\\frac{\\\\partial}{\\\\partial x_0} \\\\frac{\\\\partial \\\\varphi(t; x_0)}{\\\\partial t}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Now we use the fact that $\\\\varphi$ is a solution of our equation, so its', mode='normal'\n",
      "line='derivative with respect to time equal to the right-hand side:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\dot w(t) = \\\\frac{\\\\partial}{\\\\partial x_0} f(t, \\\\varphi(t; x_0)).', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Function $f$ does not depend on $x_0$ directly, but it depends on the solution', mode='normal'\n",
      "line='$\\\\varphi(t; x_0)$ that depends on $x_0$; thus, the chain rule should be applied:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\dot w(t)=\\\\left.\\\\frac{\\\\partial f(t, x)}{\\\\partial x}\\\\right|_{x=\\\\varphi(t; x_0)}', mode='math'\n",
      "line='\\\\frac{\\\\partial \\\\varphi(t; x_0)}{\\\\partial x_0}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='But now the second multiplier in the right-hand part is just a $w(t)$, so we', mode='normal'\n",
      "line='obtained the desired equation. End of proof.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /spoiler >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The derivative $\\\\partial g^{0:t}(x_0)/\\\\partial x_0$ is the same thing as $w(t)$ in', mode='normal'\n",
      "line='the theorem, therefore', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\frac{d}{dt} \\\\frac{\\\\partial g^{0: t}(x_0)}{\\\\partial x_0} = ', mode='math'\n",
      "line='\\\\frac{\\\\partial f(t, x)}{\\\\partial x}\\\\frac{\\\\partial g^{0: t}(x_0)}{\\\\partial x_0}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='and we have the following equation for $a(t)$:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='0 = \\\\dot a(t)\\\\frac{\\\\partial g^{0: t}(x_0)}{\\\\partial x_0}+a(t) \\\\frac{\\\\partial f(t, x)}{\\\\partial x}\\\\frac{\\\\partial g^{0: t}(x_0)}{\\\\partial x_0}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='It can be shown from theory of linear differential equations that matrix', mode='normal'\n",
      "line='$\\\\partial g^{0:t}(x_0) / \\\\partial x_0$ is nondegenerate for all $t$. Therefore,', mode='normal'\n",
      "line='we can multiply the equation by the inverse matrix and obtain:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{adjoint}', mode='math'\n",
      "line='\\\\dot a(t) = - a(t) \\\\frac{\\\\partial f(t, x)}{\\\\partial x}.', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='This! This is the adjoint equation we are looking for! In the right-hand part', mode='normal'\n",
      "line='the derivative is taken at point $x=x(t)$, i.e. along the solution of the', mode='normal'\n",
      "line='initial differential equation.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='So, we have an equation on $a$, but we know that in ODE the equation alone is not', mode='normal'\n",
      "line='enough to specify a solution. What about the initial condition? Look at $a(T)$.', mode='normal'\n",
      "line='It is a derivative of $L$ with respect to the value of the layer $T$. But the', mode='normal'\n",
      "line='layer $T$ is the output layer of the network.  Therefore, $a(T)$ is just a', mode='normal'\n",
      "line=\"derivative of $L(y, y_{true})$ with respect to $y$, where $y=x(T)$. For a given $x(T)$, the derivative does not depend on the network! And therefore it's an\", mode='normal'\n",
      "line='appropriate initial condition for the adjoint equation:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{adjoint-initial}', mode='math'\n",
      "line='a(T)=\\\\nabla_{\\\\! y} L(y, y_{true}), \\\\quad y=x(T).', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='### Backpropagation in neural ODEs', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"Let's summarize the algorithm to find loss gradient for Neural ODEs. We have\", mode='normal'\n",
      "line='an equation', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{eq}', mode='math'\n",
      "line='\\\\dot x=f_\\\\theta(t, x),', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='the input value $x_{input}$, the true output $y_{true}$ and some value of the', mode='normal'\n",
      "line='parameter vector $\\\\theta$. We want to find the gradient of the training', mode='normal'\n",
      "line='objective ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='$$\\\\mathcal L(\\\\theta) = L(x(T;\\\\theta), y_{true})$$ ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='with respect to $\\\\theta$. Here $x(t; \\\\theta)$ is a solution of equation', mode='normal'\n",
      "line='\\\\eqref{eq} with the initial condition $x(0; \\\\theta)=x_{input}$. As before, we', mode='normal'\n",
      "line='will omit the dependence on $\\\\theta$ in the following equations.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='First, we do the forward pass, i.e. solve the equation \\\\eqref{eq} numerically and find', mode='normal'\n",
      "line='$x(T)$. In the usual neural network, we store all the outputs of intermediate', mode='normal'\n",
      "line='layers $h_1, \\\\ldots, h_N$ to use them in the backward pass. In the neural ODE,', mode='normal'\n",
      "line=\"strictly speaking, it's impossible to store all the intermediate outputs,\", mode='normal'\n",
      "line='because there are infinite number of them. We can theoretically store intermediate', mode='normal'\n",
      "line='outputs at some time sequence, i.e. store $x(t_j)$ for some moments $t_j$, that', mode='normal'\n",
      "line='can be used to approximate the full trajectory.', mode='normal'\n",
      "line=\"However, it appears that we don't need it and can make our algorithms\", mode='normal'\n",
      "line='memory-efficient. So, just store $x_{output}=x(T)$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Now backward pass. We need to solve the adjoint equation \\\\eqref{adjoint} and', mode='normal'\n",
      "line='find integral \\\\eqref{nabla-L-int}. It can be a bit tricky. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"First, as before, we want to be memory-efficient and thus don't want to\", mode='normal'\n",
      "line='store the trajectories. So we need to solve the adjoint equation and integrate', mode='normal'\n",
      "line='at the same time. Moreover, the right-hand side of the adjoint equation', mode='normal'\n",
      "line='\\\\eqref{adjoint} depends on the solution of the initial equation $x(t)$, that we', mode='normal'\n",
      "line=\"didn't store. So we have to reconstruct it together with solving adjoint\", mode='normal'\n",
      "line='equation and integrating. ', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='A lot of things to do! However, it appears we can do all together by combining', mode='normal'\n",
      "line='everything into one system of ODEs:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{equation}', mode='math'\n",
      "line='\\\\label{final}', mode='math'\n",
      "line='\\\\begin{cases}', mode='math'\n",
      "line='\\\\dot x=f_\\\\theta(t, x),\\\\\\\\', mode='math'\n",
      "line='\\\\dot a = - a \\\\cdot \\\\frac{\\\\partial f_\\\\theta(t, x)}{\\\\partial x},\\\\\\\\', mode='math'\n",
      "line='\\\\dot u = - a \\\\cdot \\\\frac{\\\\partial f_\\\\theta(t, x)}{\\\\partial \\\\theta}.', mode='math'\n",
      "line='\\\\end{cases}', mode='math'\n",
      "line='\\\\end{equation}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='that should be solved backward in time with the initial conditions', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{cases}', mode='math'\n",
      "line='x(T)=x_{output} & \\\\text{(found at the forward pass)}\\\\\\\\', mode='math'\n",
      "line='a(T)=\\\\nabla_{\\\\! y} L(y, y_{true}), & y=x_{output}\\\\\\\\', mode='math'\n",
      "line='u(T)=0', mode='math'\n",
      "line='\\\\end{cases}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='The first two equations of the system \\\\eqref{final} are just equations', mode='normal'\n",
      "line='\\\\eqref{eq} and \\\\eqref{adjoint}. What about the third one? We see that its', mode='normal'\n",
      "line=\"right-hand side doesn't depend on the unkown variable $u$, so its solution\", mode='normal'\n",
      "line='(provided that we know the solutions of two other equations) is just an', mode='normal'\n",
      "line='integral:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\begin{align*}', mode='math'\n",
      "line='u(t)&=-\\\\int_{T}^{t} a(\\\\tau) \\\\cdot \\\\frac{\\\\partial f_\\\\theta(\\\\tau, x(\\\\tau))}{\\\\partial \\\\theta}', mode='math'\n",
      "line='d\\\\tau\\\\\\\\', mode='math'\n",
      "line='&=\\\\int_{t}^T a(\\\\tau) \\\\cdot \\\\frac{\\\\partial f_\\\\theta(\\\\tau, x(\\\\tau))}{\\\\partial \\\\theta}', mode='math'\n",
      "line='d\\\\tau.', mode='math'\n",
      "line='\\\\end{align*}', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='The limits of integration were chosen in such a way to satisfy the initial', mode='normal'\n",
      "line='condition $u(T)=0$. Now recall that $a(t)=\\\\nabla_{\\\\\\\\! x(t)} L$. Put it into', mode='normal'\n",
      "line='the integral above and let $t=0$: voila, we have integral \\\\eqref{nabla-L-int}! Thus', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='u(0)=\\\\nabla_{\\\\! \\\\theta} \\\\mathcal L(\\\\theta),', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='and this is exactly the value we are interested in!', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='So, in the backward pass we just solve system \\\\eqref{final} with the given', mode='normal'\n",
      "line=\"initial conditions over the segment $[0, T]$ and return $u(0)$. That's all!\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< spoiler ', mode='normal'\n",
      "line='text=\"Interested in the rigorous derivation of system \\\\eqref{final}? I have one!\" >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='The derivation is short but a bit cryptic. It uses the following well-known', mode='normal'\n",
      "line='trick: include the parameter $\\\\theta$ as a phase variable. E.g.', mode='normal'\n",
      "line='instead of equation \\\\eqref{eq}, consider the following system', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\dot x=f_\\\\theta(t, x), \\\\quad \\\\dot \\\\theta=0.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='Then consider an extended adjoint', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='\\\\vec{a}(t):=\\\\nabla_{\\\\\\\\! (x(t),\\\\\\\\, \\\\theta(t))} L = ', mode='math'\n",
      "line='(\\\\nabla_{\\\\\\\\! x(t)}L,', mode='math'\n",
      "line='\\\\nabla_{\\\\\\\\! \\\\theta(t)} L)=(\\\\nabla_{\\\\\\\\! x} (L\\\\circ g^{t:T}\\\\_\\\\theta), \\\\nabla\\\\_{\\\\\\\\! \\\\theta}', mode='math'\n",
      "line='(L\\\\circ g^{t:T}\\\\_\\\\theta)) = (a(t), u(t)),', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='i.e. $\\\\vec{a}$ is just a concatenation of vectors $a$ and $u$, the first', mode='normal'\n",
      "line='component, as before, measures how $L$ depends on the “output of the layer $t$”', mode='normal'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line='(i.e. $x(t)$), and the second component measures how $L$ depends on the', mode='normal'\n",
      "line='parameter $\\\\theta$. Both gradients are found at point $x(t)$ of the unperturbed', mode='normal'\n",
      "line='solution. In other words, while calculating $u(t)$, one considers a system that', mode='normal'\n",
      "line='works like the following: on the segment $[0, t]$, it uses the original value of', mode='normal'\n",
      "line='the parameter $\\\\theta$, and on the segment $[t, T]$ is uses the perturbed value', mode='normal'\n",
      "line='of the parameter, i.e. $\\\\theta + \\\\Delta \\\\theta$. Then $u(t)$ measures the effect', mode='normal'\n",
      "line='of $\\\\Delta \\\\theta$ on the output $x(T; \\\\theta + \\\\Delta \\\\theta)$.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Then $\\\\vec{a}$ should satisfy the adjoint equation \\\\eqref{adjoint}, where $x$ is', mode='normal'\n",
      "line='replaced with $(x, \\\\theta)$ and $f$ is replaced with $(f_{\\\\theta}(t, x), 0)$.', mode='normal'\n",
      "line='One have:', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< math >}}$$', mode='normal'\n",
      "line='(\\\\dot a, \\\\dot u) =', mode='math'\n",
      "line='-(a, u) ', mode='math'\n",
      "line='\\\\begin{pmatrix}', mode='math'\n",
      "line='\\\\frac{\\\\partial f_{\\\\theta}(t, x)}{\\\\partial x} & ', mode='math'\n",
      "line='\\\\frac{\\\\partial f_{\\\\theta}(t, x)}{\\\\partial \\\\theta} \\\\\\\\\\\\\\\\', mode='math'\n",
      "line='0 & 0', mode='math'\n",
      "line='\\\\end{pmatrix}.', mode='math'\n",
      "line='$${{< /math >}}', mode='math'\n",
      "line='', mode='end-of-math'\n",
      "line='The second row of the matrix is $0$ because the second component of our extended', mode='normal'\n",
      "line='system is $0$ and therefore its derivatives with respect to $x$ and $\\\\theta$ are', mode='normal'\n",
      "line='zeroes. Making matrix multiplication, one obtains \\\\eqref{adjoint}.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Hooray!', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='{{< /spoiler >}}', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='## Concluding remarks', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"That was a long story, and it's time to conclude. Let me reiterate several main ideas:\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='- The goal of backpropagation and adjoint state method is to find a gradient of', mode='normal'\n",
      "line='    the loss function with respect to the parameters in a computationally', mode='normal'\n",
      "line=\"    efficient way. We don't want to waste resources calculating more than\", mode='normal'\n",
      "line='    needed, so the order of operations matters.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='- These methods are based on a simple idea: when you have a composition of', mode='normal'\n",
      "line='    several functions such that the last function in the composition ', mode='normal'\n",
      "line='    takes values in one-dimensional space (and therefore the full composition do', mode='normal'\n",
      "line='    the same), the derivative of the output of such a composition with respect to any', mode='normal'\n",
      "line='    intermediate output is just a vector-row (covector), and not a full matrix.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='- This idea can be naturally extended to continuous settings, where instead of', mode='normal'\n",
      "line='    a long composition we have a differential equation.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='- Both in discrete and continuous settings there are effective algorithms to', mode='normal'\n",
      "line='    calculate the derivatives of the one-dimensional output with respect to', mode='normal'\n",
      "line='    intermediate values. These algorithms work “backward”: from the last', mode='normal'\n",
      "line=\"    intermediate “layers” to the first ones. In discrete settings, it's the\", mode='normal'\n",
      "line='    reccurrence \\\\eqref{nablastep} ', mode='normal'\n",
      "line=\"    (also known as \\\\eqref{nablaLstep}). In continous settings, it's the\", mode='normal'\n",
      "line='    adjoint equation \\\\eqref{adjoint}.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='- These algorithms efficiently reuse the derivative they found at the', mode='normal'\n",
      "line=\"    previous “steps” and do not waste time calculating things that don't needed.\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='- It is possible to adapt these algorithms to settings when you need a', mode='normal'\n",
      "line='    derivative of the output with respect to the parameters, as we have in the neural', mode='normal'\n",
      "line='    networks.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line='- To do that efficiently, we have to disentangle two effects: 1. Change of the', mode='normal'\n",
      "line='    output of some intermediate layer due to change of the parameters; 2. Change', mode='normal'\n",
      "line='    of the output of the subsequent layers due to change of the output of the', mode='normal'\n",
      "line='    intermediate layer. Then we have to integrate over all intermediate layers,', mode='normal'\n",
      "line='    and that leads to the solution.', mode='normal'\n",
      "line='', mode='normal'\n",
      "line=\"That's all! \", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='Did you enjoy this post? Follow me on', mode='normal'\n",
      "line=\"[Twitter](https://twitter.com/ilya_schurov) and let's stay in touch!\", mode='normal'\n",
      "line='', mode='normal'\n",
      "line='[^1]: Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud. [Neural Ordinary Differential Equations](https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf). 32nd Conference on Neural Information Processing Systems (NeurIPS 2018).', mode='normal'\n"
     ]
    }
   ],
   "source": [
    "chunk = []\n",
    "mode = 'start'\n",
    "out = []\n",
    "for line in lines:\n",
    "    print(f\"{line=}, {mode=}\")\n",
    "    if mode == 'start':\n",
    "        out.append(line)\n",
    "        if line.strip() == \"---\":\n",
    "            mode = 'preamble'\n",
    "            continue\n",
    "    if mode == 'preamble':\n",
    "        out.append(line)\n",
    "        if line.strip() == \"---\":\n",
    "            mode = 'normal'\n",
    "            continue\n",
    "    if mode == \"normal\":\n",
    "        line = line.strip()\n",
    "        if line == \"\":\n",
    "            out.append(re.sub(\"\\s+\", \" \", \" \".join(chunk)).strip())\n",
    "            out.append(\"\")\n",
    "            chunk.clear()\n",
    "            continue\n",
    "        if re.match(r\"{{< math >}}(\\$\\$)?$\", line):\n",
    "            assert \"\".join(chunk).strip() == \"\", \"Math is glued to the text: \" + \"\".join(chunk)\n",
    "            out.append(line)\n",
    "            mode = 'math'\n",
    "            continue\n",
    "        chunk.append(line)\n",
    "    if mode == \"math\":\n",
    "        out.append(line)\n",
    "        if re.match(r\"(\\$\\$)?{{< /math >}}$\", line.strip()):\n",
    "            mode = 'end-of-math'\n",
    "            continue\n",
    "    if mode == 'end-of-math':\n",
    "        assert line.strip() == \"\", \"Math is glued to the text: \" + line\n",
    "        out.append(line)\n",
    "        mode = 'normal'\n",
    "        continue\n",
    "out.append(re.sub(\"\\s+\", \" \", \" \".join(chunk)).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "continental-plaza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70300"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\"./index.md\").write_text(\"\\n\".join(out) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-reset",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-primary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
